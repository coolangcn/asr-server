#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, sys, logging, json, threading, subprocess, time, traceback, tempfile, argparse
import numpy as np
from scipy.spatial.distance import cosine
from flask import Flask, request, jsonify, render_template, send_file, send_from_directory, Response
from funasr import AutoModel  # ASR ç”¨ FunASR
from modelscope.pipelines import pipeline  # SV ç”¨ ModelScope
from modelscope.utils.constant import Tasks
import torch
import torchaudio
import shutil
import re
from collections import Counter
from db_manager import save_to_db, update_topics, parse_recording_time, init_pool, init_db
from logging.handlers import TimedRotatingFileHandler
import whisper
import requests
import hashlib
from datetime import datetime

# =================ã€ é…ç½® ã€‘=================
class Config:
    DEVICE = "cuda:0"
    HOST = '0.0.0.0'
    PORT = 5008
    SPEAKER_DB_FILE = "speaker_db_multi.json"    
    # é•¿å¥éŸ³é¢‘ä¿å­˜é…ç½®
    SAVE_LONG_SENTENCES = True  # æ˜¯å¦ä¿å­˜é•¿å¥éŸ³é¢‘
    MIN_TEXT_LENGTH_TO_SAVE = 15  # æœ€å°‘å­—æ•°
    LONG_SENTENCES_DIR = "long_sentences"  # ä¿å­˜ç›®å½•
    TEMP_DIR = "temp"  # ä¸´æ—¶æ–‡ä»¶ç›®å½•
    
    ONLY_REGISTERED_SPEAKERS = True  # åªä¿ç•™å·²æ³¨å†Œè¯´è¯äºº,ä¸¢å¼ƒUnknown
    # ASRæ¨¡å‹é…ç½® - Paraformer (æ”¯æŒVADåˆ†æ®µå’Œè¯´è¯äººåˆ†ç¦»)
    ASR_MODEL = "iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"  # ä» SenseVoiceSmall åˆ‡æ¢åˆ° Paraformer
    VAD_MODEL = "fsmn-vad"       # VADæ¨¡å‹
    SPK_MODEL = "cam++"          # è¯´è¯äººåˆ†ç¦»æ¨¡å‹  
    PUNC_MODEL = "ct-punc"       # æ ‡ç‚¹æ¢å¤æ¨¡å‹
    
    # VADå‚æ•°é…ç½®(ä¸ºParaformerä¼˜åŒ–)
    VAD_MAX_SINGLE_SEGMENT = 15000  # ms - å•æ®µæœ€é•¿æ—¶é—´
    VAD_MAX_END_SILENCE = 300       # ms - æ®µå°¾é™éŸ³é˜ˆå€¼
    VAD_SIL_TO_SPEECH = 50          # ms - é™éŸ³åˆ°è¯­éŸ³é˜ˆå€¼
    VAD_SPEECH_TO_SIL = 80          # ms - è¯­éŸ³åˆ°é™éŸ³é˜ˆå€¼
    
    SV_MODELS = {
        "eres2net_large": {
            "id": "iic/speech_eres2net_large_200k_sv_zh-cn_16k-common",
            "rev": "v1.0.0",
            "threshold": 0.50,  # æé«˜é˜ˆå€¼ä»¥æé«˜å‡†ç¡®ç‡
            "gap": 0.08         # é™ä½ç½®ä¿¡åº¦é—´éš”è¦æ±‚
        },
        "rdino_ecapa": {
            "id": "iic/speech_rdino_ecapa_tdnn_sv_zh-cn_cnceleb_16k",
            "rev": "v1.0.0",
            "threshold": 0.50,  # æé«˜é˜ˆå€¼ä»¥æé«˜å‡†ç¡®ç‡
            "gap": 0.08         # é™ä½ç½®ä¿¡åº¦é—´éš”è¦æ±‚
        },
        "camplusplus": {
            "id": "iic/speech_campplus_sv_zh-cn_16k-common",
            "rev": "v1.0.0",
            "threshold": 0.50,  # æé«˜é˜ˆå€¼ä»¥æé«˜å‡†ç¡®ç‡
            "gap": 0.08         # é™ä½ç½®ä¿¡åº¦é—´éš”è¦æ±‚
        }
    }
    
    MIN_SPEAKER_DURATION_MS = 800
    NORMALIZE_AUDIO = True
    DENOISE_AUDIO = False  # å¯ç”¨é«˜çº§é™å™ª
    
    # å¯é€‰åŠŸèƒ½å¼€å…³
    ENABLE_EMOTION_DETECTION = True  # æ˜¯å¦å¯ç”¨æƒ…æ„Ÿæ£€æµ‹(éœ€è¦SenseVoiceæ¨¡å‹)
    ENABLE_WHISPER_COMPARISON = True  # æ˜¯å¦å¯ç”¨Whisperå¯¹æ¯”(éœ€è¦Whisperæ¨¡å‹)
    
    # SenseVoiceé…ç½® (æƒ…æ„Ÿæ£€æµ‹)
    SENSEVOICE_MODEL = "iic/SenseVoiceSmall"
    ENABLE_SENSEVOICE = True  # æ˜¯å¦å¯ç”¨SenseVoice(æƒ…æ„Ÿæ£€æµ‹+ç¬¬ä¸‰è½¬å½•)

# æ–‡ä»¶ç›‘æ§é…ç½®
class FileMonitorConfig:
    SOURCE_DIR = "V:\\Sony-2"
    PROCESSED_DIR = "processed"
    FAILED_DIR = "failed"
    TRANSCRIPTS_DIR = "transcripts"
    SCAN_INTERVAL = 3  # ç§’
    SUPPORTED_FORMATS = ['.m4a', '.mp3', '.wav', '.aac', '.flac', '.ogg', '.acc']

# LLM é…ç½®
class LLMConfig:
    USE_GEMINI_LLM = False
    GEMINI_API_KEY = "cncncncn"
    GEMINI_API_BASE_URL = "https://gl.moco.fun/proxy/gemini"
    GEMINI_MODEL_NAME = "gemini-2.5-flash"
    
    # æ‰¹é‡å¤„ç†é…ç½®
    LLM_BATCH_MODE = True
    LLM_BATCH_SIZE = 20
    LLM_BATCH_TIMEOUT = 600  # 10åˆ†é’Ÿ
    
    # è¿‡æ»¤æ¡ä»¶
    LLM_MIN_TEXT_LENGTH = 50
    LLM_MIN_SEGMENTS = 3
    LLM_CACHE_SIZE = 100
    LLM_REQUEST_TIMEOUT = 30
# ==========================================

EMOTION_TAGS = {
    "<|happy|>": "happy", "<|sad|>": "sad", "<|angry|>": "angry",
    "<|neutral|>": "neutral", "<|laughter|>": "laughter", "<|fearful|>": "fearful",
    "<|disgusted|>": "disgusted", "<|surprised|>": "surprised", "<|EMO_UNKNOWN|>": "neutral"
}
INVALID_TAGS = {"<|nospeech|>", "<|BGM|>", "<|Event_UNK|>", "<|music|>"}

# æ–°å¢ï¼šå®šä¹‰è¯´è¯äººæ•°æ®ç»“æ„
# {
#   "speaker_name": {
#     "samples": [
#       {
#         "id": "sample_id",
#         "filename": "file_name.wav",
#         "timestamp": "2023-01-01 12:00:00",
#         "embeddings": {
#           "eres2net_large": [...],
#           "rdino_ecapa": [...]
#         }
#       }
#     ],
#     "avg_embeddings": {
#       "eres2net_large": [...],
#       "rdino_ecapa": [...]
#     }
#   }
# }

# åˆ›å»ºæ—¥å¿—é˜Ÿåˆ—ç”¨äºSSE
export_logger = logging.getLogger('export_logger')
export_logger.setLevel(logging.INFO)

# è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—æ¶ˆæ¯å‘é€åˆ°SSEè¿æ¥
class SSEHandler(logging.Handler):
    def __init__(self):
        super().__init__()
        self.clients = set()
    
    def add_client(self, client):
        self.clients.add(client)
    
    def remove_client(self, client):
        self.clients.remove(client)
    
    def emit(self, record):
        msg = self.format(record)
        for client in list(self.clients):
            try:
                client.write(f"data: {json.dumps({'message': msg, 'level': record.levelname})}\n\n")
            except Exception:
                self.remove_client(client)

# åˆ›å»ºæ—¥å¿—å¤„ç†å™¨
log_formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)
console_handler.setLevel(logging.INFO)

# åˆ›å»ºå¹¶é…ç½®SSEå¤„ç†å™¨
sse_handler = SSEHandler()
sse_handler.setFormatter(log_formatter)
sse_handler.setLevel(logging.INFO)

# é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨ï¼Œç”¨äºå°†æ—¥å¿—å†™å…¥æ–‡ä»¶ï¼ˆæ¯10åˆ†é’Ÿè½®è½¬ä¸€æ¬¡ï¼‰
# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs("log", exist_ok=True)
file_handler = TimedRotatingFileHandler(
    'log/asr-server.log', 
    when='M',           # æŒ‰åˆ†é’Ÿè½®è½¬
    interval=10,        # æ¯10åˆ†é’Ÿ
    backupCount=144,    # ä¿ç•™144ä¸ªæ–‡ä»¶ï¼ˆ24å°æ—¶ï¼‰
    encoding='utf-8'
)
file_handler.setFormatter(log_formatter)
file_handler.setLevel(logging.INFO)
logger.addHandler(file_handler) # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨

logger.addHandler(console_handler)
logger.addHandler(sse_handler)

app = Flask(__name__)
logging.getLogger('werkzeug').setLevel(logging.ERROR)

asr_pipeline = None
sv_pipelines = {}
speaker_db = {}
emotion_pipeline = None  # å¯é€‰: æƒ…æ„Ÿæ£€æµ‹æ¨¡å‹
whisper_model = None     # å¯é€‰: Whisperå¯¹æ¯”æ¨¡å‹
sensevoice_pipeline = None  # å¯é€‰: SenseVoiceæ¨¡å‹(æƒ…æ„Ÿ+è½¬å½•)
gpu_lock = threading.Lock()
db_lock = threading.Lock()

# =================ã€ LLM æ‰¹é‡å¤„ç†å…¨å±€å˜é‡ ã€‘=================
llm_batch_queue = []
llm_batch_lock = threading.Lock()
llm_last_batch_time = time.time()
llm_cache = {}  # ç¼“å­˜ LLM å“åº”
llm_cache_lock = threading.Lock()
# =========================================================

# =================== æ¨¡å‹åŠ è½½ ===================
def load_models():
    global asr_pipeline, sv_pipelines, whisper_model, sensevoice_pipeline
    print("\n====== ğŸš€ å¯åŠ¨ SOTA èåˆæœåŠ¡ ======")
    
    load_speaker_db()

    # 2. åŠ è½½ ASR (FunASR)
    print(f"ğŸ§  åŠ è½½ ASR: {Config.ASR_MODEL} ...")
    # 2. åŠ è½½ ASR (FunASR Paraformer + VAD + è¯´è¯äººåˆ†ç¦»)
    print(f"ğŸ§  åŠ è½½ ASR: {Config.ASR_MODEL} (æ”¯æŒVADåˆ†æ®µå’Œè¯´è¯äººåˆ†ç¦») ...")
    asr_pipeline = AutoModel(
        model=Config.ASR_MODEL,       # paraformer-zh
        vad_model=Config.VAD_MODEL,   # fsmn-vad
        punc_model=Config.PUNC_MODEL, # ct-punc (æ ‡ç‚¹æ¢å¤)
        spk_model=Config.SPK_MODEL,   # cam++ (è¯´è¯äººåˆ†ç¦»)
        vad_kwargs={
            "max_single_segment_time": Config.VAD_MAX_SINGLE_SEGMENT,
            "max_end_silence_time": Config.VAD_MAX_END_SILENCE,
            "sil_to_speech_time_thres": Config.VAD_SIL_TO_SPEECH,
            "speech_to_sil_time_thres": Config.VAD_SPEECH_TO_SIL
        },
        device=Config.DEVICE, 
        disable_update=True
    )
    print("âœ… Paraformeræ¨¡å‹åŠ è½½å®Œæˆï¼Œå·²å¯ç”¨VADåˆ†æ®µå’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½")

    # 3. åŠ è½½ SV æ¨¡å‹
    for name, conf in Config.SV_MODELS.items():
        print(f"ğŸ” åŠ è½½ SV [{name}] : {conf['id']} ...")
        sv_pipelines[name] = pipeline(
            task=Tasks.speaker_verification,
            model=conf['id'], 
            model_revision=conf['rev'], 
            device=Config.DEVICE.split(':')[0]
        )
    print(f"âœ… æœåŠ¡å°±ç»ª | ASR: SenseVoice | SV: {list(sv_pipelines.keys())}\n")

    # 4. åŠ è½½ Whisper æ¨¡å‹ (å¯é€‰)
    if Config.ENABLE_WHISPER_COMPARISON:
        print(f"ğŸ¤ åŠ è½½ Whisper large-v3 æ¨¡å‹ (æœ€æ–°æœ€ä½³ç‰ˆæœ¬,éœ€è¦~10GBæ˜¾å­˜)...")

        try:
            whisper_model = whisper.load_model("large-v3", device=Config.DEVICE.split(':')[0])
            print("âœ… Whisper large-v3 æ¨¡å‹åŠ è½½å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ç¦ç”¨Whisperå¯¹æ¯”åŠŸèƒ½")
            whisper_model = None

    # 5. åŠ è½½ SenseVoice æ¨¡å‹ (æƒ…æ„Ÿæ£€æµ‹)
    if Config.ENABLE_SENSEVOICE:
        print(f"ğŸ­ åŠ è½½ SenseVoice æ¨¡å‹ (æƒ…æ„Ÿæ£€æµ‹+ç¬¬ä¸‰è½¬å½•)...")
        try:
            sensevoice_pipeline = AutoModel(
                model=Config.SENSEVOICE_MODEL,
                device=Config.DEVICE
            )
            print("âœ… SenseVoice æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ SenseVoiceæ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ç¦ç”¨SenseVoiceåŠŸèƒ½")
            sensevoice_pipeline = None

# =================ã€ æ™ºèƒ½æ‘˜è¦å’Œ LLM å‡½æ•° ã€‘=================

def generate_conversation_summary(segments, audio_duration):
    """ç”Ÿæˆå¯¹è¯æ™ºèƒ½æ‘˜è¦"""
    if not segments:
        return None
    
    # ç»Ÿè®¡è¯´è¯äºº
    speaker_stats = {}
    for seg in segments:
        speaker = seg.get('spk', 'Unknown')
        if speaker not in speaker_stats:
            speaker_stats[speaker] = {'count': 0, 'total_duration': 0, 'word_count': 0}
        speaker_stats[speaker]['count'] += 1
        speaker_stats[speaker]['total_duration'] += (seg.get('end', 0) - seg.get('start', 0)) / 1000.0
        speaker_stats[speaker]['word_count'] += len(seg.get('text', ''))
    
    # æå–é«˜é¢‘è¯
    stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'æœ‰', 'ä¸ª', 'å°±', 'ä¸', 'å’Œ', 'ä¸'}
    all_text = ''.join([seg.get('text', '') for seg in segments])
    words = [all_text[i:i+2] for i in range(len(all_text)-1)]
    word_freq = Counter([w for w in words if w not in stop_words and len(w) == 2])
    top_keywords = [word for word, count in word_freq.most_common(5)]
    
    # æƒ…æ„Ÿç»Ÿè®¡
    emotion_stats = Counter([seg.get('emotion') for seg in segments if seg.get('emotion')])
    
    return {
        'total_segments': len(segments),
        'total_duration': round(audio_duration, 2),
        'speaker_count': len(speaker_stats),
        'speakers': speaker_stats,
        'keywords': top_keywords,
        'emotions': dict(emotion_stats),
        'avg_segment_duration': round(audio_duration / len(segments), 2) if segments else 0
    }

def call_gemini_api(prompt):
    """è°ƒç”¨ Gemini API"""
    if not LLMConfig.USE_GEMINI_LLM:
        return None
        
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = hashlib.md5(prompt.encode()).hexdigest()
        with llm_cache_lock:
            if cache_key in llm_cache:
                logger.info(f"  [LLM] ä½¿ç”¨ç¼“å­˜å“åº”")
                return llm_cache[cache_key]
        
        url = f"{LLMConfig.GEMINI_API_BASE_URL}/v1beta/models/{LLMConfig.GEMINI_MODEL_NAME}:generateContent"
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": LLMConfig.GEMINI_API_KEY
        }
        data = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"temperature": 0.3, "maxOutputTokens": 500}
        }
        
        response = requests.post(url, headers=headers, json=data, timeout=LLMConfig.LLM_REQUEST_TIMEOUT)
        response.raise_for_status()
        result = response.json()
        
        if 'candidates' in result and len(result['candidates']) > 0:
            text = result['candidates'][0]['content']['parts'][0]['text']
            
            # ä¿å­˜åˆ°ç¼“å­˜
            with llm_cache_lock:
                if len(llm_cache) >= LLMConfig.LLM_CACHE_SIZE:
                    llm_cache.pop(next(iter(llm_cache)))
                llm_cache[cache_key] = text
            
            return text
        return None
    except Exception as e:
        logger.error(f"  [LLM] API è°ƒç”¨å¤±è´¥: {e}")
        return None

def extract_conversation_topics(full_text, segments):
    """æå–å¯¹è¯ä¸»é¢˜"""
    try:
        speakers = list(set([seg.get('spk', 'Unknown') for seg in segments]))
        speaker_text = ', '.join(speakers[:3])
        
        prompt = f"""åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

å¯¹è¯å†…å®¹ï¼š
{full_text[:500]}

è¯´è¯äººï¼š{speaker_text}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "topics": ["ä¸»é¢˜1", "ä¸»é¢˜2"],
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
  "sentiment": "positive/neutral/negative",
  "summary": "ä¸€å¥è¯æ€»ç»“"
}}"""
        
        response_text = call_gemini_api(prompt)
        if not response_text:
            return None
        
        # è§£æ JSON
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        return None
    except Exception as e:
        logger.warning(f"  [LLM] ä¸»é¢˜æå–å¤±è´¥: {e}")
        return None

def add_to_llm_queue(filename, full_text, segments):
    """æ·»åŠ åˆ° LLM æ‰¹é‡å¤„ç†é˜Ÿåˆ—"""
    global llm_last_batch_time
    
    with llm_batch_lock:
        llm_batch_queue.append({
            'filename': filename,
            'full_text': full_text,
            'segments': segments
        })
        
        queue_size = len(llm_batch_queue)
        time_since_last = time.time() - llm_last_batch_time
        
        logger.info(f"  [LLMé˜Ÿåˆ—] å·²æ·»åŠ ï¼Œå½“å‰é˜Ÿåˆ—: {queue_size}/{LLMConfig.LLM_BATCH_SIZE}")
        
        # è§¦å‘æ‰¹é‡å¤„ç†
        if queue_size >= LLMConfig.LLM_BATCH_SIZE or time_since_last >= LLMConfig.LLM_BATCH_TIMEOUT:
            logger.info(f"  [LLMé˜Ÿåˆ—] è§¦å‘æ‰¹é‡å¤„ç† (é˜Ÿåˆ—={queue_size}, è¶…æ—¶={time_since_last:.0f}s)")
            threading.Thread(target=process_llm_batch, daemon=True).start()

def process_llm_batch():
    """æ‰¹é‡å¤„ç† LLM ä»»åŠ¡"""
    global llm_last_batch_time
    
    if not LLMConfig.USE_GEMINI_LLM:
        with llm_batch_lock:
            llm_batch_queue.clear()
        return
        
    with llm_batch_lock:
        if not llm_batch_queue:
            return
        
        batch = llm_batch_queue.copy()
        llm_batch_queue.clear()
        llm_last_batch_time = time.time()
    
    logger.info(f"  [LLMæ‰¹å¤„ç†] å¼€å§‹å¤„ç† {len(batch)} æ¡è®°å½•")
    
    for item in batch:
        try:
            topics = extract_conversation_topics(item['full_text'], item['segments'])
            if topics:
                update_topics(item['filename'], topics)
                logger.info(f"  [LLM] {item['filename']}: ä¸»é¢˜={topics.get('topics', [])}")
        except Exception as e:
            logger.error(f"  [LLM] å¤„ç†å¤±è´¥ {item['filename']}: {e}")
    
    logger.info(f"  [LLMæ‰¹å¤„ç†] å®Œæˆ")

# =========================================================

def cleanup_temp_dir():
    """æ¸…ç†è¶…è¿‡1å°æ—¶çš„ä¸´æ—¶æ–‡ä»¶"""
    try:
        temp_dir = Config.TEMP_DIR
        if not os.path.exists(temp_dir):
            return
        
        current_time = time.time()
        cleaned_count = 0
        
        for filename in os.listdir(temp_dir):
            filepath = os.path.join(temp_dir, filename)
            if os.path.isfile(filepath):
                try:
                    file_age = current_time - os.path.getmtime(filepath)
                    if file_age > 3600:  # 1å°æ—¶
                        os.remove(filepath)
                        cleaned_count += 1
                        logger.debug(f"æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶: {filename}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        if cleaned_count > 0:
            logger.info(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ–‡ä»¶")
        
        # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
        threading.Timer(3600, cleanup_temp_dir).start()
    except Exception as e:
        logger.error(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")
        # å³ä½¿å¤±è´¥ä¹Ÿè¦ç»§ç»­å®šæ—¶ä»»åŠ¡
        threading.Timer(3600, cleanup_temp_dir).start()

def load_speaker_db():
    global speaker_db
    with db_lock:
        if os.path.exists(Config.SPEAKER_DB_FILE):
            try:
                with open(Config.SPEAKER_DB_FILE, 'r', encoding='utf-8') as f:
                    loaded_db = json.load(f)
                
                # å…¼å®¹æ—§æ•°æ®ç»“æ„
                converted_db = {}
                for name, data in loaded_db.items():
                    if "samples" in data and "avg_embeddings" in data:
                        # æ–°æ•°æ®ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨
                        converted_db[name] = data
                    else:
                        # æ—§æ•°æ®ç»“æ„ï¼Œè½¬æ¢ä¸ºæ–°ç»“æ„
                        logger.info(f"ğŸ”„ è½¬æ¢æ—§æ•°æ®ç»“æ„ for speaker: {name}")
                        converted_db[name] = {
                            "samples": [],  # æ—§æ•°æ®ç»“æ„æ²¡æœ‰æ ·æœ¬ä¿¡æ¯
                            "avg_embeddings": data  # æ—§æ•°æ®ç»“æ„ç›´æ¥æ˜¯åµŒå…¥å­—å…¸
                        }
                
                speaker_db = converted_db
                logger.info(f"ğŸ“š å£°çº¹åº“å·²æŒ‚è½½: {len(speaker_db)} äºº")
            except Exception as e:
                logger.error(f"å£°çº¹åº“æŸå: {e}")
                speaker_db = {}
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {Config.SPEAKER_DB_FILE}ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®åº“ã€‚")
            speaker_db = {}

# =================== éŸ³é¢‘é¢„å¤„ç† ===================
def preprocess_audio(input_path, output_path):
    # å¦‚æœå¯ç”¨äº†é«˜çº§é™å™ªï¼Œå…ˆè¿›è¡Œé™å™ªå¤„ç†
    if Config.DENOISE_AUDIO:
        denoised_path = input_path + ".denoised.wav"
        if advanced_denoise(input_path, denoised_path):
            input_path = denoised_path
        else:
            logger.warning("é«˜çº§é™å™ªå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘")
    
    cmd = ["ffmpeg", "-v", "error", "-y", "-i", input_path]
    filters = ["loudnorm=I=-14:TP=-1.5:LRA=11"] if Config.NORMALIZE_AUDIO else []
    if filters: cmd.extend(["-af", ",".join(filters)])
    cmd.extend(["-ac", "1", "-ar", "16000", output_path])
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL)
        # æ¸…ç†ä¸´æ—¶é™å™ªæ–‡ä»¶
        if Config.DENOISE_AUDIO and input_path.endswith(".denoised.wav"):
            try:
                os.remove(input_path)
            except:
                pass
        return True
    except Exception as e:
        logger.error(f"FFmpeg é¢„å¤„ç†å¤±è´¥: {e}")
        return False

def advanced_denoise(input_path, output_path):
    """ä½¿ç”¨è°±å‡æ³•è¿›è¡Œé«˜çº§é™å™ª"""
    try:
        # åŠ è½½éŸ³é¢‘
        waveform, sample_rate = torchaudio.load(input_path)
        
        # å¦‚æœé‡‡æ ·ç‡ä¸æ˜¯16kHzï¼Œå…ˆè¿›è¡Œé‡é‡‡æ ·
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resampler(waveform)
            sample_rate = 16000
        
        # è½¬æ¢ä¸ºå•å£°é“
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # ç®€åŒ–çš„è°±å‡æ³•é™å™ª
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        audio_np = waveform.numpy()[0]
        
        # è®¡ç®—çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
        from scipy import signal
        frequencies, times, Zxx = signal.stft(audio_np, fs=sample_rate, nperseg=512)
        
        # ä¼°è®¡å™ªå£°è°±ï¼ˆå‡è®¾å‰100msä¸ºå™ªå£°ï¼‰
        noise_seg_len = min(int(0.1 * sample_rate), len(audio_np))
        noise_segment = audio_np[:noise_seg_len]
        _, _, noise_stft = signal.stft(noise_segment, fs=sample_rate, nperseg=512)
        noise_spectrum = np.mean(np.abs(noise_stft), axis=1)
        
        # åº”ç”¨è°±å‡æ³•
        magnitude = np.abs(Zxx)
        phase = np.angle(Zxx)
        
        # å‡å»å™ªå£°è°±çš„ä¼°è®¡å€¼
        noise_factor = 1.5
        magnitude_denoised = np.maximum(magnitude - noise_factor * noise_spectrum[:, np.newaxis], 0)
        
        # é‡æ„ä¿¡å·
        Zxx_denoised = magnitude_denoised * np.exp(1j * phase)
        _, audio_denoised = signal.istft(Zxx_denoised, fs=sample_rate)
        
        # è£å‰ªåˆ°åŸå§‹é•¿åº¦
        audio_denoised = audio_denoised[:len(audio_np)]
        
        # ä¿å­˜é™å™ªåçš„éŸ³é¢‘
        waveform_denoised = torch.tensor(audio_denoised).unsqueeze(0)
        torchaudio.save(output_path, waveform_denoised, sample_rate)
        
        return True
    except Exception as e:
        logger.error(f"é«˜çº§é™å™ªå¤„ç†å¤±è´¥: {e}")
        return False

def extract_segment(source_path, start_ms, end_ms, output_path):
    if start_ms >= end_ms: return False
    start_sec = start_ms / 1000.0
    duration = (end_ms - start_ms) / 1000.0
    cmd = ["ffmpeg", "-v", "error", "-y", "-ss", f"{start_sec:.3f}", "-t", f"{duration:.3f}", "-i", source_path, "-ac", "1", "-ar", "16000", output_path]
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL)
        return True
    except:
        return False



def transcribe_with_whisper(audio_path):
    """
    ä½¿ç”¨Whisperè¯†åˆ«éŸ³é¢‘ç‰‡æ®µï¼ˆä½œä¸ºFunASRçš„å¯¹æ¯”å‚è€ƒï¼‰
    
    Args:
        audio_path: éŸ³é¢‘ç‰‡æ®µè·¯å¾„
        
    Returns:
        str: Whisperè¯†åˆ«çš„æ–‡æœ¬ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    if not Config.ENABLE_WHISPER_COMPARISON or whisper_model is None:
        return None
    
    try:
        result = whisper_model.transcribe(
            audio_path,
            language='zh',
            fp16=True,  # GPUåŠ é€Ÿ
            verbose=False
        )
        whisper_text = result['text'].strip()
        logger.info(f"      [Whisperå¯¹æ¯”] {whisper_text}")
        return whisper_text
    except Exception as e:
        logger.warning(f"      [Whisperå¯¹æ¯”] è¯†åˆ«å¤±è´¥: {e}")
        return None


def transcribe_with_sensevoice(audio_path):
    """
    ä½¿ç”¨SenseVoiceè¯†åˆ«éŸ³é¢‘å¹¶æ£€æµ‹æƒ…æ„Ÿ
    
    Returns:
        tuple: (text, emotion) - è¯†åˆ«æ–‡æœ¬å’Œæƒ…æ„Ÿ
    """
    if not Config.ENABLE_SENSEVOICE or sensevoice_pipeline is None:
        return None, None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿè¿”å›None
    
    try:
        result = sensevoice_pipeline.generate(
            input=audio_path,
            language="auto",
            use_itn=True
        )
        
        if not result or len(result) == 0:
            return None, None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿè¿”å›None
        
        raw_text = result[0].get("text", "")
        
        # æå–æƒ…æ„Ÿ
        emotion = None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿæ—¶ä¸ºNone,ä¸ä½¿ç”¨neutral
        for tag, emo_code in EMOTION_TAGS.items():
            if tag.lower() in raw_text.lower():
                emotion = emo_code
                break
        
        # ç§»é™¤æƒ…æ„Ÿæ ‡ç­¾
        clean_text = re.sub(r'<\|.*?\|>', '', raw_text).strip()
        
        logger.info(f"      [SenseVoice] {clean_text} (æƒ…æ„Ÿ: {emotion})")
        return clean_text, emotion
        
    except Exception as e:
        logger.warning(f"      [SenseVoice] è¯†åˆ«å¤±è´¥: {e}")
        return None, None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿè¿”å›None

def detect_emotion_for_segment(audio_path):
    """ä½¿ç”¨SenseVoiceæ£€æµ‹éŸ³é¢‘æ®µçš„æƒ…æ„Ÿ"""
    if not Config.ENABLE_EMOTION_DETECTION or emotion_pipeline is None:
        return "neutral"
    
    try:
        result = emotion_pipeline(
            audio_in=audio_path,
            language="auto",
            use_itn=True
        )
        
        if not result or len(result) == 0:
            return "neutral"
        
        raw_text = result[0].get("text", "")
        logger.info(f"      [SenseVoiceæƒ…æ„Ÿ] åŸå§‹è¾“å‡º: {raw_text}")
        
        # æå–æƒ…æ„Ÿæ ‡ç­¾
        emotion = None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿæ—¶ä¸ºNone,ä¸ä½¿ç”¨neutral
        raw_text_lower = raw_text.lower()
        
        EMOTION_MAP = {
            '<|happy|>': 'happy',
            '<|sad|>': 'sad', 
            '<|angry|>': 'angry',
            '<|neutral|>': 'neutral',
            '<|fearful|>': 'fearful',
            '<|disgusted|>': 'disgusted',
            '<|surprised|>': 'surprised'
        }
        
        for tag, emo in EMOTION_MAP.items():
            if tag in raw_text_lower:
                emotion = emo
                logger.info(f"      [SenseVoiceæƒ…æ„Ÿ] æ£€æµ‹åˆ°æƒ…æ„Ÿ: {emotion}")
                break
        
        return emotion
    except Exception as e:
        logger.warning(f"      [SenseVoiceæƒ…æ„Ÿ] æ£€æµ‹å¤±è´¥: {e}")
        return "neutral"


# =================== æå– embedding ===================
def extract_embedding_from_file(sv_pipe, wav_path):
    try:
        model = sv_pipe.model
        audio, sr = torchaudio.load(wav_path)
        if sr != 16000:
            resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
            audio = resample(audio)
        
        audio = audio.mean(dim=0, keepdim=True) # [C, T] -> [1, T]

        with torch.no_grad():
            out = model(audio)
            if isinstance(out, dict):
                emb = out.get("spk_embedding")
            else:
                emb = out
        return emb.squeeze().cpu().numpy()

    except Exception as e:
        logger.error(f"âŒ extract_embedding å¤±è´¥: {e}")
        return None

# =================== å¤šæ¨¡å‹äº¤å‰éªŒè¯ ===================
def identify_speaker_fusion(segment_path):
    if not speaker_db: 
        logger.info("ğŸ¤·â€â™‚ï¸ å£°çº¹æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè¯†åˆ«")
        return None, 0.0, []

    model_votes = {}
    model_scores = {}

    logger.info(f"ğŸ¯ å¼€å§‹å£°çº¹è¯†åˆ«: éŸ³é¢‘æ®µè·¯å¾„={segment_path}")
    logger.info(f"ğŸ“‹ å£°çº¹æ•°æ®åº“åŒ…å« {len(speaker_db)} ä¸ªè¯´è¯äºº")

    for model_name, sv_pipe in sv_pipelines.items():
        # æ¨¡å‹å¤„ç†ï¼ˆé™é»˜ï¼‰
        
        emb_a = extract_embedding_from_file(sv_pipe, segment_path)
        if emb_a is None:
            logger.error(f"âŒ æ¨¡å‹ {model_name} ç‰¹å¾æå–å¤±è´¥")
            model_votes[model_name] = "Failed"
            continue

        scores = []
        conf = Config.SV_MODELS[model_name]
        threshold = conf['threshold']
        gap = conf['gap']
        # é…ç½®å·²åœ¨å¯åŠ¨æ—¶æ˜¾ç¤ºï¼Œæ— éœ€é‡å¤

        for name, speaker_data in speaker_db.items():
            # ä½¿ç”¨å¹³å‡åµŒå…¥è¿›è¡Œæ¯”è¾ƒ
            if "avg_embeddings" not in speaker_data or model_name not in speaker_data["avg_embeddings"]: 
                continue
            emb_b = np.array(speaker_data["avg_embeddings"][model_name]).flatten()
            score = 1 - cosine(emb_a.flatten(), emb_b)
            scores.append((name, score))
            # DEBUG: è¯¦ç»†è¯„åˆ†
            logger.debug(f"  {model_name}: {name}={score:.3f}")

        if not scores:
            logger.warning(f"âš ï¸ æ¨¡å‹ {model_name} æœªæ‰¾åˆ°åŒ¹é…çš„è¯´è¯äººæ•°æ®")
            model_votes[model_name] = "NoDB"
            continue

        scores.sort(key=lambda x: x[1], reverse=True)
        top1_name, top1_score = scores[0]
        top2_name, top2_score = scores[1] if len(scores) > 1 else (None, 0.0)
        score_gap = top1_score - top2_score
        
        # DEBUG: æ¨¡å‹è¯†åˆ«ç»“æœ
        logger.debug(f"  {model_name}: {top1_name}={top1_score:.3f} (gap={score_gap:.3f})")

        if top1_score >= threshold and score_gap >= gap:
            model_votes[model_name] = top1_name
            model_scores[model_name] = top1_score
            # éªŒè¯é€šè¿‡ï¼ˆé™é»˜ï¼‰
            pass
        else:
            model_votes[model_name] = "Unknown"
            model_scores[model_name] = top1_score
            reason = []
            if top1_score < threshold:
                reason.append(f"å¾—åˆ† {top1_score:.6f} < é˜ˆå€¼ {threshold}")
            if score_gap < gap:
                reason.append(f"å·®è· {score_gap:.6f} < ç½®ä¿¡åº¦é—´éš” {gap}")
            # éªŒè¯å¤±è´¥ï¼ˆé™é»˜ï¼‰
            pass

    # DEBUG: æŠ•ç¥¨ç»“æœ
    logger.debug(f"  æŠ•ç¥¨: {model_votes}")
    
    # 2/3æŠ•ç¥¨é€»è¾‘
    votes = [v for v in model_votes.values() if v not in ["Unknown", "Failed", "NoDB"]]
    if not votes:
        # è¯†åˆ«å¤±è´¥ï¼ˆç”±ä¸Šå±‚è®°å½•ï¼‰
        logger.debug("  æœªè¯†åˆ«: æ‰€æœ‰æ¨¡å‹å‡æœªé€šè¿‡")
        return None, 0.0, []

    vote_counts = Counter(votes)
    most_common_vote = vote_counts.most_common(1)[0]
    winner, count = most_common_vote
    
    # è‡³å°‘éœ€è¦2ç¥¨
    if count >= 2:
        # è®¡ç®—è·èƒœè€…çš„å¹³å‡ç½®ä¿¡åº¦
        winning_scores = [model_scores[model] for model, vote in model_votes.items() if vote == winner]
        avg_confidence = np.mean(winning_scores)
        
        # è¯†åˆ«æˆåŠŸï¼ˆç”±ä¸Šå±‚è®°å½•ï¼‰
        logger.debug(f"  è¯†åˆ«: {winner} (ç½®ä¿¡åº¦={avg_confidence:.3f}, ç¥¨æ•°={count})")
        
        # ç”Ÿæˆè¯¦ç»†ä¿¡æ¯
        recognition_details = []
        for model_name, result in model_votes.items():
            if result in ["Unknown", "Failed", "NoDB"]:
                recognition_details.append(f"æ¨¡å‹ {model_name}: {result}")
            else:
                recognition_details.append(f"æ¨¡å‹ {model_name}: è¯†åˆ«ä¸º {result} (ç›¸ä¼¼åº¦: {model_scores.get(model_name, 0):.6f})")
        recognition_details.append(f"æœ€ç»ˆè¯†åˆ«ç»“æœ: {winner} (å¤šæ•°ç¥¨: {count} ç¥¨, å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f})")
        
        return winner, avg_confidence, recognition_details
    else:
        # ç”Ÿæˆè¯†åˆ«å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯
        recognition_details = []
        for model_name, result in model_votes.items():
            recognition_details.append(f"æ¨¡å‹ {model_name}: {result} (ç›¸ä¼¼åº¦: {model_scores.get(model_name, 0):.6f})")
        recognition_details.append("æœ€ç»ˆè¯†åˆ«ç»“æœ: è¯†åˆ«å¤±è´¥ï¼Œæ²¡æœ‰å€™é€‰äººè·å¾—è¶³å¤Ÿç¥¨æ•° (å¤šæ•°ç¥¨ â‰¥ 2)")
        
        # è¯†åˆ«å¤±è´¥ï¼ˆç”±ä¸Šå±‚è®°å½•ï¼‰
        logger.debug(f"  æœªè¯†åˆ«: ç¥¨æ•°ä¸è¶³ ({winner}={count}<2)")
        return None, 0.0, []

# =================== Flask æ¥å£ ===================
@app.route("/")
def home():
    return render_template("register.html")

@app.route("/register_page")
def register_page():
    return render_template("register.html")

@app.route("/manage")
def manage_page():
    return render_template("manage.html")

@app.route("/speakers", methods=["GET"])
def get_speakers():
    """è·å–æ‰€æœ‰è¯´è¯äººåˆ—è¡¨"""
    try:
        # é‡æ–°åŠ è½½å£°çº¹æ•°æ®åº“ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„
        load_speaker_db()
        # è¿”å›è¯´è¯äººåˆ—è¡¨ï¼ˆä¸åŒ…å«å…·ä½“çš„embeddingæ•°æ®ï¼‰
        speakers_summary = {}
        for name, data in speaker_db.items():
            sample_count = len(data.get("samples", []))
            model_names = list(data.get("avg_embeddings", {}).keys())
            speakers_summary[name] = {
                "sample_count": sample_count,
                "models": model_names
            }
        return jsonify({"speakers": speakers_summary})
    except Exception as e:
        logger.error(f"è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to retrieve speakers"}), 500

@app.route("/speaker/<speaker_name>", methods=["GET"])
def get_speaker_samples(speaker_name):
    """è·å–æŒ‡å®šè¯´è¯äººçš„æ ·æœ¬åˆ—è¡¨"""
    try:
        # é‡æ–°åŠ è½½å£°çº¹æ•°æ®åº“ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„
        load_speaker_db()
        if speaker_name not in speaker_db:
            return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
            
        speaker_data = speaker_db[speaker_name]
        # è¿”å›æ ·æœ¬ä¿¡æ¯ï¼ˆä¸åŒ…å«å…·ä½“çš„embeddingæ•°æ®ï¼‰
        samples_info = []
        for sample in speaker_data.get("samples", []):
            samples_info.append({
                "id": sample["id"],
                "filename": sample["filename"],
                "timestamp": sample["timestamp"]
            })
        
        return jsonify({
            "speaker_name": speaker_name,
            "sample_count": len(samples_info),
            "samples": samples_info,
            "models": list(speaker_data.get("avg_embeddings", {}).keys())
        })
    except Exception as e:
        logger.error(f"è·å–è¯´è¯äººæ ·æœ¬åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to retrieve speaker samples"}), 500

@app.route("/speaker/<speaker_name>", methods=["DELETE"])
def delete_speaker(speaker_name):
    """åˆ é™¤æŒ‡å®šè¯´è¯äºº"""
    try:
        with db_lock:
            if speaker_name in speaker_db:
                del speaker_db[speaker_name]
                # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
                with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(speaker_db, f, indent=2, ensure_ascii=False)
                logger.info(f"âœ… æˆåŠŸåˆ é™¤è¯´è¯äºº: {speaker_name}")
                return jsonify({"message": f"Speaker '{speaker_name}' deleted successfully."})
            else:
                return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
    except Exception as e:
        logger.error(f"åˆ é™¤è¯´è¯äººå¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to delete speaker"}), 500

@app.route("/speaker/<speaker_name>/sample/<sample_id>", methods=["DELETE"])
def delete_speaker_sample(speaker_name, sample_id):
    """åˆ é™¤æŒ‡å®šè¯´è¯äººçš„ç‰¹å®šæ ·æœ¬"""
    try:
        with db_lock:
            if speaker_name not in speaker_db:
                return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
                
            speaker_data = speaker_db[speaker_name]
            if "samples" not in speaker_data:
                return jsonify({"error": f"No samples found for speaker '{speaker_name}'."}), 404
                
            # æŸ¥æ‰¾å¹¶åˆ é™¤æŒ‡å®šæ ·æœ¬
            samples = speaker_data["samples"]
            sample_to_remove = None
            sample_index = -1
            for i, sample in enumerate(samples):
                if sample["id"] == sample_id:
                    sample_to_remove = sample
                    sample_index = i
                    break
                    
            if sample_to_remove is None:
                return jsonify({"error": f"Sample '{sample_id}' not found for speaker '{speaker_name}'."}), 404
                
            # åˆ é™¤æ ·æœ¬çš„éŸ³é¢‘æ–‡ä»¶
            if "audio_path" in sample_to_remove and os.path.exists(sample_to_remove["audio_path"]):
                try:
                    os.remove(sample_to_remove["audio_path"])
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†éŸ³é¢‘æ–‡ä»¶: {sample_to_remove['audio_path']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ é™¤éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {sample_to_remove['audio_path']}, é”™è¯¯: {str(e)}")
            
            # ä»æ•°æ®åº“ä¸­ç§»é™¤æ ·æœ¬è®°å½•
            del samples[sample_index]
                
            # å¦‚æœåˆ é™¤æ ·æœ¬åæ²¡æœ‰å‰©ä½™æ ·æœ¬ï¼Œåˆ™åˆ é™¤æ•´ä¸ªè¯´è¯äºº
            if len(samples) == 0:
                del speaker_db[speaker_name]
                # åˆ é™¤è¯´è¯äººçš„ç›®å½•
                speaker_dir = os.path.join("speaker_samples", speaker_name)
                if os.path.exists(speaker_dir):
                    try:
                        shutil.rmtree(speaker_dir)
                        logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†è¯´è¯äººç›®å½•: {speaker_dir}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤è¯´è¯äººç›®å½•å¤±è´¥: {speaker_dir}, é”™è¯¯: {str(e)}")
                
                with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(speaker_db, f, indent=2, ensure_ascii=False)
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†è¯´è¯äºº {speaker_name}ï¼ˆæœ€åä¸€ä¸ªæ ·æœ¬å·²åˆ é™¤ï¼‰")
                return jsonify({"message": f"Speaker '{speaker_name}' deleted (last sample removed)."})

            # é‡æ–°è®¡ç®—å¹³å‡åµŒå…¥
            all_model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}
            for sample in samples:
                for model_name, emb in sample["embeddings"].items():
                    all_model_embeddings[model_name].append(np.array(emb))
            
            # è®¡ç®—æ–°çš„å¹³å‡åµŒå…¥
            new_avg_embeddings = {}
            for model_name, emb_list in all_model_embeddings.items():
                if emb_list:
                    avg_emb = np.mean(emb_list, axis=0)
                    new_avg_embeddings[model_name] = avg_emb.tolist()
            
            speaker_db[speaker_name]["avg_embeddings"] = new_avg_embeddings
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
            with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                json.dump(speaker_db, f, indent=2, ensure_ascii=False)
                
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†è¯´è¯äºº {speaker_name} çš„æ ·æœ¬ {sample_id}")
            return jsonify({
                "message": f"Sample '{sample_id}' deleted from speaker '{speaker_name}'.",
                "remaining_samples": len(samples)
            })
    except Exception as e:
        logger.error(f"åˆ é™¤è¯´è¯äººæ ·æœ¬å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to delete speaker sample"}), 500


@app.route("/speaker/list", methods=["GET"])
def list_speakers():
    """è·å–æ‰€æœ‰è¯´è¯äººåˆ—è¡¨ (Web Vieweræ ¼å¼)"""
    try:
        # é‡æ–°åŠ è½½å£°çº¹æ•°æ®åº“ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„
        load_speaker_db()
        # è¿”å›è¯´è¯äººåˆ—è¡¨æ•°ç»„æ ¼å¼
        speakers_list = []
        for name, data in speaker_db.items():
            sample_count = len(data.get("samples", []))
            speakers_list.append({
                "name": name,
                "sample_count": sample_count
            })
        return jsonify({"speakers": speakers_list})
    except Exception as e:
        logger.error(f"è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to retrieve speakers"}), 500

@app.route("/speaker/register", methods=["POST"])
def register_speaker_web():
    """æ³¨å†Œå£°çº¹ (Web Vieweræ ¼å¼) - é€‚é…å™¨ç«¯ç‚¹"""
    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
    os.makedirs(Config.TEMP_DIR, exist_ok=True)
    temp_files = []
    with gpu_lock:
        try:
            if 'speaker_name' not in request.form or not request.form['speaker_name']:
                return jsonify({"error": "Speaker name is required"}), 400
            
            speaker_name = request.form['speaker_name']
            
            # Web viewerå‘é€å•ä¸ªaudio_fileï¼Œéœ€è¦è½¬æ¢ä¸ºaudio_filesåˆ—è¡¨
            if 'audio_file' not in request.files:
                return jsonify({"error": "Audio file is required"}), 400
            
            audio_file = request.files['audio_file']
            
            # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦å¢å¼ºæ¨¡å¼
            enhance_mode = speaker_name in speaker_db

            action = "å¢å¼º" if enhance_mode else "æ³¨å†Œ"
            logger.info(f"ğŸ“¥ å¼€å§‹{action}æ–°å£°çº¹: {speaker_name} | æ–‡ä»¶: {audio_file.filename}")
            
            # åˆ›å»ºè¯´è¯äººæ ·æœ¬ç›®å½•
            speaker_dir = os.path.join("speaker_samples", speaker_name)
            if not os.path.exists(speaker_dir):
                os.makedirs(speaker_dir)
            
            # æ”¶é›†æ–°æ ·æœ¬æ•°æ®
            new_samples = []
            model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}

            # å¤„ç†éŸ³é¢‘æ–‡ä»¶
            raw_temp = os.path.join(Config.TEMP_DIR, f"reg_raw_{int(time.time())}_{audio_file.filename}")
            audio_file.save(raw_temp)
            temp_files.append(raw_temp)
            
            proc_temp = os.path.join(Config.TEMP_DIR, f"reg_proc_{int(time.time())}.wav")
            temp_files.append(proc_temp)

            if not preprocess_audio(raw_temp, proc_temp):
                return jsonify({"error": f"Audio preprocessing failed for {audio_file.filename}"}), 500

            # ä¸ºæ¯ä¸ªæ¨¡å‹æå–åµŒå…¥
            sample_embeddings = {}
            for model_name, sv_pipe in sv_pipelines.items():
                emb = extract_embedding_from_file(sv_pipe, proc_temp)
                if emb is not None:
                    sample_embeddings[model_name] = emb.tolist()
                    model_embeddings[model_name].append(emb)
                else:
                    logger.warning(f"âš ï¸ ä» {audio_file.filename} æå– {model_name} embedding å¤±è´¥ã€‚")

            # ä¿å­˜æ ·æœ¬ä¿¡æ¯å’ŒéŸ³é¢‘æ–‡ä»¶
            if not sample_embeddings:
                return jsonify({"error": "Failed to extract embeddings from audio file"}), 500
            
            # ç”Ÿæˆå”¯ä¸€çš„æ ·æœ¬ID
            sample_id = f"{int(time.time())}_{hash(audio_file.filename) % 10000}"
            
            # ä¿å­˜å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶
            sample_audio_path = os.path.join(speaker_dir, f"{sample_id}.wav")
            shutil.copy2(proc_temp, sample_audio_path)
            
            sample_info = {
                "id": sample_id,
                "filename": audio_file.filename,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "audio_path": sample_audio_path,
                "embeddings": sample_embeddings
            }
            new_samples.append(sample_info)

            # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡åµŒå…¥
            avg_embeddings = {}
            for model_name, emb_list in model_embeddings.items():
                if not emb_list:
                    continue
                avg_emb = np.mean(emb_list, axis=0)
                avg_embeddings[model_name] = avg_emb.tolist()
                logger.info(f"  - æ¨¡å‹ [{model_name}] å¤„ç†äº† {len(emb_list)} ä¸ªæ ·æœ¬")

            if not avg_embeddings:
                return jsonify({"error": "Failed to extract embeddings from any samples"}), 500

            with db_lock:
                # å¦‚æœè¯´è¯äººå·²å­˜åœ¨ï¼Œåˆ™æ·»åŠ æ–°æ ·æœ¬å¹¶æ›´æ–°å¹³å‡åµŒå…¥
                if enhance_mode and speaker_name in speaker_db:
                    # æ·»åŠ æ–°æ ·æœ¬åˆ°ç°æœ‰æ ·æœ¬åˆ—è¡¨
                    if "samples" not in speaker_db[speaker_name]:
                        speaker_db[speaker_name]["samples"] = []
                    speaker_db[speaker_name]["samples"].extend(new_samples)
                    
                    # é‡æ–°è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡åµŒå…¥
                    all_model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}
                    
                    # æ·»åŠ ç°æœ‰æ ·æœ¬çš„åµŒå…¥
                    for sample in speaker_db[speaker_name]["samples"]:
                        for model_name, emb in sample["embeddings"].items():
                            all_model_embeddings[model_name].append(np.array(emb))
                    
                    # é‡æ–°è®¡ç®—å¹³å‡åµŒå…¥
                    new_avg_embeddings = {}
                    for model_name, emb_list in all_model_embeddings.items():
                        if emb_list:
                            new_avg_embeddings[model_name] = np.mean(emb_list, axis=0).tolist()
                    
                    speaker_db[speaker_name]["avg_embeddings"] = new_avg_embeddings
                    total_samples = len(speaker_db[speaker_name]["samples"])
                    logger.info(f"âœ… æˆåŠŸå¢å¼ºè¯´è¯äºº [{speaker_name}]ï¼Œå½“å‰å…± {total_samples} ä¸ªæ ·æœ¬")
                else:
                    # æ–°å»ºè¯´è¯äºº
                    speaker_db[speaker_name] = {
                        "samples": new_samples,
                        "avg_embeddings": avg_embeddings
                    }
                    logger.info(f"âœ… æˆåŠŸæ³¨å†Œæ–°è¯´è¯äºº [{speaker_name}]ï¼Œå…± {len(new_samples)} ä¸ªæ ·æœ¬")

                # ä¿å­˜åˆ°æ•°æ®åº“æ–‡ä»¶
                with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(speaker_db, f, indent=2, ensure_ascii=False)

            return jsonify({
                "message": f"Speaker '{speaker_name}' {'enhanced' if enhance_mode else 'registered'} successfully.",
                "sample_count": len(speaker_db[speaker_name]["samples"])
            })

        except Exception as e:
            logger.error(f"æ³¨å†Œå£°çº¹å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({"error": str(e)}), 500
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for tmp in temp_files:
                try:
                    if os.path.exists(tmp):
                        os.remove(tmp)
                except:
                    pass

@app.route("/register", methods=["POST"])
def register_speaker():
    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
    os.makedirs(Config.TEMP_DIR, exist_ok=True)
    temp_files = []
    with gpu_lock:
        try:
            if 'speaker_name' not in request.form or not request.form['speaker_name']:
                return jsonify({"error": "Speaker name is required"}), 400
            
            speaker_name = request.form['speaker_name']
            audio_files = request.files.getlist('audio_files')
            
            # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦å¢å¼ºæ¨¡å¼
            enhance_mode = speaker_name in speaker_db

            if not audio_files:
                return jsonify({"error": "At least one audio file is required"}), 400

            action = "å¢å¼º" if enhance_mode else "æ³¨å†Œ"
            logger.info(f"ğŸ“¥ å¼€å§‹{action}æ–°å£°çº¹: {speaker_name} | æ–‡ä»¶æ•°: {len(audio_files)}")
            
            # åˆ›å»ºè¯´è¯äººæ ·æœ¬ç›®å½•
            speaker_dir = os.path.join("speaker_samples", speaker_name)
            if not os.path.exists(speaker_dir):
                os.makedirs(speaker_dir)
            
            # æ”¶é›†æ–°æ ·æœ¬æ•°æ®
            new_samples = []
            model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}

            for file in audio_files:
                raw_temp = os.path.join(Config.TEMP_DIR, f"reg_raw_{int(time.time())}_{file.filename}")
                file.save(raw_temp)
                temp_files.append(raw_temp)
                
                proc_temp = os.path.join(Config.TEMP_DIR, f"reg_proc_{int(time.time())}.wav")
                temp_files.append(proc_temp)

                if not preprocess_audio(raw_temp, proc_temp):
                    logger.warning(f"âš ï¸ æ–‡ä»¶ {file.filename} é¢„å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
                    continue

                # ä¸ºæ¯ä¸ªæ¨¡å‹æå–åµŒå…¥
                sample_embeddings = {}
                for model_name, sv_pipe in sv_pipelines.items():
                    emb = extract_embedding_from_file(sv_pipe, proc_temp)
                    if emb is not None:
                        sample_embeddings[model_name] = emb.tolist()
                        model_embeddings[model_name].append(emb)
                    else:
                        logger.warning(f"âš ï¸ ä» {file.filename} æå– {model_name} embedding å¤±è´¥ã€‚")

                # ä¿å­˜æ ·æœ¬ä¿¡æ¯å’ŒéŸ³é¢‘æ–‡ä»¶
                if sample_embeddings:  # åªæœ‰å½“è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹æˆåŠŸæå–åµŒå…¥æ—¶æ‰ä¿å­˜æ ·æœ¬
                    # ç”Ÿæˆå”¯ä¸€çš„æ ·æœ¬ID
                    sample_id = f"{int(time.time())}_{hash(file.filename) % 10000}"
                    
                    # ä¿å­˜å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶
                    sample_audio_path = os.path.join(speaker_dir, f"{sample_id}.wav")
                    shutil.copy2(proc_temp, sample_audio_path)
                    
                    sample_info = {
                        "id": sample_id,
                        "filename": file.filename,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "audio_path": sample_audio_path,
                        "embeddings": sample_embeddings
                    }
                    new_samples.append(sample_info)

            # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡åµŒå…¥
            avg_embeddings = {}
            for model_name, emb_list in model_embeddings.items():
                if not emb_list:
                    continue
                avg_emb = np.mean(emb_list, axis=0)
                avg_embeddings[model_name] = avg_emb.tolist()
                logger.info(f"  - æ¨¡å‹ [{model_name}] å¤„ç†äº† {len(emb_list)} ä¸ªæ ·æœ¬")

            if not avg_embeddings:
                return jsonify({"error": "Failed to extract embeddings from any samples"}), 500

            with db_lock:
                # å¦‚æœè¯´è¯äººå·²å­˜åœ¨ï¼Œåˆ™æ·»åŠ æ–°æ ·æœ¬å¹¶æ›´æ–°å¹³å‡åµŒå…¥
                if enhance_mode and speaker_name in speaker_db:
                    # æ·»åŠ æ–°æ ·æœ¬åˆ°ç°æœ‰æ ·æœ¬åˆ—è¡¨
                    if "samples" not in speaker_db[speaker_name]:
                        speaker_db[speaker_name]["samples"] = []
                    speaker_db[speaker_name]["samples"].extend(new_samples)
                    
                    # é‡æ–°è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡åµŒå…¥
                    all_model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}
                    
                    # æ·»åŠ ç°æœ‰æ ·æœ¬çš„åµŒå…¥
                    for sample in speaker_db[speaker_name]["samples"]:
                        for model_name, emb in sample["embeddings"].items():
                            all_model_embeddings[model_name].append(np.array(emb))
                    
                    # é‡æ–°è®¡ç®—å¹³å‡åµŒå…¥
                    new_avg_embeddings = {}
                    for model_name, emb_list in all_model_embeddings.items():
                        if emb_list:
                            avg_emb = np.mean(emb_list, axis=0)
                            new_avg_embeddings[model_name] = avg_emb.tolist()
                    
                    speaker_db[speaker_name]["avg_embeddings"] = new_avg_embeddings
                    logger.info(f"ğŸ”„ å¢å¼ºäº†è¯´è¯äºº {speaker_name} çš„å£°çº¹ï¼Œæ–°å¢ {len(new_samples)} ä¸ªæ ·æœ¬")
                else:
                    # åˆ›å»ºæ–°çš„è¯´è¯äººæ¡ç›®
                    speaker_db[speaker_name] = {
                        "samples": new_samples,
                        "avg_embeddings": avg_embeddings
                    }
                    logger.info(f"ğŸ†• åˆ›å»ºäº†æ–°è¯´è¯äºº {speaker_name} çš„å£°çº¹ï¼ŒåŒ…å« {len(new_samples)} ä¸ªæ ·æœ¬")
                    
                # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
                with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(speaker_db, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… å£°çº¹{action}æˆåŠŸ: {speaker_name}")
            return jsonify({
                "message": f"Speaker '{speaker_name}' {action} successfully.",
                "samples_added": len(new_samples)
            })

        except Exception as e:
            logger.error(f"âŒ æ³¨å†Œå¼‚å¸¸: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({"error": "An internal error occurred during registration."} ), 500
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œä½†ä¿ç•™è¯­éŸ³ç‰‡æ®µæ–‡ä»¶ä¾›webç«¯é¢„è§ˆä½¿ç”¨
            for f in temp_files:
                if os.path.exists(f):
                    # ä¸åˆ é™¤è¯­éŸ³ç‰‡æ®µæ–‡ä»¶ (seg_*.wav)ï¼Œè¿™äº›æ–‡ä»¶éœ€è¦ä¿ç•™ä¾›webç«¯é¢„è§ˆ
                    if os.path.basename(f).startswith("seg_"):
                        logger.info(f"  [ä¿ç•™] è¯­éŸ³ç‰‡æ®µæ–‡ä»¶ä¾›é¢„è§ˆä½¿ç”¨: {os.path.basename(f)}")
                        continue
                    try: os.remove(f)
                    except: pass


@app.route("/transcribes", methods=["POST"])
def transcribe_audio():
    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
    os.makedirs(Config.TEMP_DIR, exist_ok=True)
    request_start = time.time()
    temp_files = []

    with gpu_lock:
        try:
            if 'audio_file' not in request.files: return jsonify({"error": "No file uploaded"}), 400
            
            file = request.files['audio_file']
            
            # å¿½ç•¥åŒ…å« TEMP çš„æ–‡ä»¶å (é™é»˜è·³è¿‡,ä¸å¤„ç†)
            if 'TEMP' in file.filename:
                logger.info(f"â­ï¸ å¿½ç•¥ä¸´æ—¶æ–‡ä»¶: {file.filename}")
                return jsonify({
                    "message": "Temporary file ignored",
                    "filename": file.filename,
                    "full_text": "",
                    "segments": [],
                    "meta": {"ignored": True}
                }), 200
            
            raw_temp = os.path.join(Config.TEMP_DIR, f"raw_{int(time.time())}_{file.filename}")
            file.save(raw_temp)
            temp_files.append(raw_temp)
            proc_temp = os.path.join(Config.TEMP_DIR, f"proc_{int(time.time())}.wav")
            temp_files.append(proc_temp)
            
            logger.info(f"ğŸ“¥ æ”¶åˆ°è½¬å½•ä»»åŠ¡: {file.filename}")
            
            logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 1. éŸ³é¢‘é¢„å¤„ç†] å¼€å§‹ (FFmpegé™å™ªã€é‡é‡‡æ ·ã€å½’ä¸€åŒ–)...")
            if not preprocess_audio(raw_temp, proc_temp):
                return jsonify({"error": "Audio preprocessing failed"}), 500
            logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 1. éŸ³é¢‘é¢„å¤„ç†] å®Œæˆã€‚")

            audio_duration = 0
            try:
                probe = subprocess.check_output(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', proc_temp])
                audio_duration = float(probe)
            except: pass

            logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 2. VAD & ASR] å¼€å§‹ (FunASRè¯­éŸ³æ£€æµ‹ä¸æ–‡å­—è½¬å½•)...")
            res = asr_pipeline.generate(input=proc_temp, language="auto", use_itn=True, use_punc=True)
            # logger.info(f"  [VAD è°ƒè¯•] FunASR generate() åŸå§‹è¿”å›: {json.dumps(res, ensure_ascii=False, indent=2)}")
            full_text = ""
            segments = []
            processed_segments = []

            if res and isinstance(res, list) and len(res) > 0:
                item = res[0]
                full_text = item.get("text", "")
                
                raw_segments = item.get("sentence_info", [])
                logger.info(f"  [ç”Ÿå‘½å‘¨æœŸ: 2. VAD & ASR] å®Œæˆ, VADæ£€å‡º {len(raw_segments)} ä¸ªåˆ†æ®µã€‚")

                if not raw_segments and full_text:
                    raw_segments = [{"text": full_text, "start": 0, "end": int(audio_duration * 1000)}]

                processed_segments = []
                
                if raw_segments:
                    logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 3. é€æ®µå£°çº¹è¯†åˆ«] å¼€å§‹...")
                    for i, seg in enumerate(raw_segments):
                        raw_text = seg.get("text", "")
                        start, end = seg.get("start", 0), seg.get("end", 0)
                        logger.info(f"    [3.{i+1}] å¤„ç†åˆ†æ®µ {start}ms - {end}ms...")
                        
                        if any(tag in raw_text for tag in INVALID_TAGS): continue

                        # Case-insensitive emotion detection
                        emotion = None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿæ—¶ä¸ºNone,ä¸ä½¿ç”¨neutral
                        raw_text_lower = raw_text.lower()
                        for tag, emo_code in EMOTION_TAGS.items():
                            if tag.lower() in raw_text_lower:
                                emotion = emo_code
                                if "laughter" in tag.lower():
                                    emotion = "laughter" # Prioritize laughter
                                    break
                        if "<|cry|>" in raw_text_lower:
                            emotion = "sad"

                        # Case-insensitive, universal tag removal
                        clean_text = re.sub(r'<\|.*?\|>', '', raw_text).replace(" ", "").strip()
                        if not clean_text: 
                            logger.info(f"      [3.{i+1}] åˆ†æ®µæ–‡æœ¬åœ¨æ¸…æ´—åä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
                            continue

                        identity, confidence = None, 0.0
                        recognition_details = []

                        segment_audio_path = None

                        if (end - start) > Config.MIN_SPEAKER_DURATION_MS:
                            # åˆ›å»ºæŒä¹…åŒ–çš„éŸ³é¢‘ç‰‡æ®µç›®å½•
                            # ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åå’Œ_TEMPåç¼€ï¼‰ä½œä¸ºå­ç›®å½•
                            original_filename = file.filename.replace('_TEMP', '')  # ç§»é™¤_TEMPåç¼€
                            base_filename = os.path.splitext(original_filename)[0]
                            segments_dir = os.path.join(FileMonitorConfig.SOURCE_DIR, "audio_segments", base_filename)
                            os.makedirs(segments_dir, exist_ok=True)
                            
                            # ä¸´æ—¶æ–‡ä»¶ç”¨äºå¤„ç†
                            seg_wav_temp = os.path.join(Config.TEMP_DIR, f"seg_{start}_{i}_{int(time.time())}.wav")
                            # æŒä¹…åŒ–æ–‡ä»¶
                            seg_filename = f"seg_{i}.wav"
                            seg_wav_persistent = os.path.join(segments_dir, seg_filename)
                            
                            if extract_segment(proc_temp, start, end, seg_wav_temp):
                                temp_files.append(seg_wav_temp)
                                
                                # å¤åˆ¶åˆ°æŒä¹…åŒ–ç›®å½•
                                try:
                                    shutil.copy2(seg_wav_temp, seg_wav_persistent)
                                    # åªæœ‰æˆåŠŸå¤åˆ¶åæ‰ä¿å­˜è·¯å¾„
                                    segment_audio_path = f"/audio_segments/{base_filename}/{seg_filename}"
                                    logger.debug(f"      [éŸ³é¢‘ç‰‡æ®µ] å·²ä¿å­˜: {seg_wav_persistent}")
                                except Exception as copy_error:
                                    logger.error(f"      [éŸ³é¢‘ç‰‡æ®µ] å¤åˆ¶å¤±è´¥: {copy_error}")
                                    segment_audio_path = None  # å¦‚æœå¤åˆ¶å¤±è´¥,ä¸è®¾ç½®è·¯å¾„

                                identity, confidence, recognition_details = identify_speaker_fusion(seg_wav_temp)
                                
                                # æ€§èƒ½ä¼˜åŒ–: åªæœ‰è¯†åˆ«å‡ºçš„è¯´è¯äººæ‰è¿›è¡ŒWhisperå’ŒSenseVoiceå¤„ç†
                                if identity is not None:
                                    # æƒ…æ„Ÿæ£€æµ‹
                                    emotion = detect_emotion_for_segment(seg_wav_temp)
                                    # Whisperå¯¹æ¯”è¯†åˆ«
                                    whisper_text = transcribe_with_whisper(seg_wav_temp)
                                    
                                    # SenseVoiceè¯†åˆ«å’Œæƒ…æ„Ÿæ£€æµ‹
                                    sensevoice_text, sensevoice_emotion = transcribe_with_sensevoice(seg_wav_temp)
                                    
                                    # ä½¿ç”¨SenseVoiceçš„æƒ…æ„Ÿç»“æœ(å¦‚æœæ£€æµ‹åˆ°)
                                    if sensevoice_emotion is not None:
                                        emotion = sensevoice_emotion
                                    
                                    logger.info(f"      [æ€§èƒ½] å·²è¯†åˆ«è¯´è¯äºº {identity}, å®Œæ•´å¤„ç†")
                                else:
                                    # Unknownè¯´è¯äººè·³è¿‡é¢å¤–å¤„ç†
                                    logger.info(f"      [æ€§èƒ½] æœªè¯†åˆ«è¯´è¯äºº, è·³è¿‡Whisper/SenseVoiceå¤„ç†")
                                    whisper_text = None
                                    sensevoice_text = None
                                    emotion = None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿæ—¶ä¸ºNone,ä¸ä½¿ç”¨neutral
                                
                                # ä¿å­˜è¶…è¿‡15ä¸ªå­—çš„è¯­å¥éŸ³é¢‘
                                # æ£€æµ‹æ˜¯å¦ä¸ºå™ªéŸ³(é‡å¤å­—ç¬¦è¿‡å¤š)
                # æ£€æµ‹æ˜¯å¦ä¸ºå™ªéŸ³(é‡å¤å­—ç¬¦è¿‡å¤šæˆ–å¡«å……è¯)
                                def is_noise(text):
                                    if not text:
                                        return True
                                    # æ£€æµ‹å•å­—ç¬¦é‡å¤ç‡
                                    from collections import Counter
                                    char_counts = Counter(text)
                                    most_common_char, most_common_count = char_counts.most_common(1)[0]
                                    repeat_ratio = most_common_count / len(text)
                                    # å¦‚æœæŸä¸ªå­—ç¬¦å æ¯”è¶…è¿‡40%,è®¤ä¸ºæ˜¯å™ªéŸ³
                                    if repeat_ratio > 0.4:
                                        return True
                                    
                                    # æ£€æµ‹å¡«å……è¯(å—¯ã€å•Šã€å‘ƒç­‰)
                                    filler_words = ['å—¯', 'å•Š', 'å‘ƒ', 'é¢', 'å“¦', 'å””']
                                    # ç§»é™¤æ ‡ç‚¹åæ£€æŸ¥
                                    text_no_punct = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼Ÿ,.!?]', '', text)
                                    if not text_no_punct:
                                        return True
                                    # è®¡ç®—å¡«å……è¯å æ¯”
                                    filler_count = sum(text_no_punct.count(w) for w in filler_words)
                                    filler_ratio = filler_count / len(text_no_punct)
                                    # å¦‚æœå¡«å……è¯å æ¯”è¶…è¿‡60%,è®¤ä¸ºæ˜¯å™ªéŸ³
                                    return filler_ratio > 0.6


                                # åªä¿å­˜å·²è¯†åˆ«è¯´è¯äººçš„é•¿å¥å­(è·³è¿‡Unknown)
                                if Config.SAVE_LONG_SENTENCES and identity is not None and len(clean_text) >= Config.MIN_TEXT_LENGTH_TO_SAVE and not is_noise(clean_text):
                                    try:
                                        os.makedirs(Config.LONG_SENTENCES_DIR, exist_ok=True)
                                        timestamp = int(time.time())
                                        speaker_name = identity  # å·²ç¡®ä¿identityä¸ä¸ºNone
                                        saved_filename = f"{timestamp}_{speaker_name}_{len(clean_text)}chars.wav"
                                        saved_path = os.path.join(Config.LONG_SENTENCES_DIR, saved_filename)
                                        shutil.copy2(seg_wav_temp, saved_path)
                                        
                                        # åŒæ—¶ä¿å­˜æ–‡æœ¬ä¿¡æ¯
                                        txt_path = saved_path.replace('.wav', '.txt')
                                        with open(txt_path, 'w', encoding='utf-8') as f:
                                            f.write(f"è¯´è¯äºº: {speaker_name}\n")
                                            f.write(f"æ–‡æœ¬é•¿åº¦: {len(clean_text)} å­—\n")
                                            f.write(f"æ—¶é—´: {start}ms - {end}ms\n")
                                            f.write(f"æƒ…æ„Ÿ: {emotion}\n")
                                            f.write(f"ç½®ä¿¡åº¦: {confidence:.3f}\n")
                                            f.write(f"\n=== FunASR è¯†åˆ«ç»“æœ ===\n{clean_text}\n")
                                            if whisper_text:
                                                f.write(f"\n=== Whisper è¯†åˆ«ç»“æœ ===\n{whisper_text}\n")
                                            if sensevoice_text:
                                                f.write(f"\n=== SenseVoice è¯†åˆ«ç»“æœ ===\n{sensevoice_text}\n")
                                        
                                        logger.info(f"      [é•¿å¥ä¿å­˜] å·²ä¿å­˜ {len(clean_text)} å­—éŸ³é¢‘: {saved_filename}")
                                    except Exception as e:
                                        logger.warning(f"      [é•¿å¥ä¿å­˜] ä¿å­˜å¤±è´¥: {e}")
                        else:
                            logger.info(f"      [3.{i+1}] åˆ†æ®µæ—¶é•¿è¿‡çŸ­({end-start}ms)ï¼Œè·³è¿‡å£°çº¹è¯†åˆ«ã€‚")
                            # å³ä½¿è·³è¿‡å£°çº¹è¯†åˆ«ï¼Œä¹Ÿè¦åˆå§‹åŒ–è¿™äº›å˜é‡
                            emotion = None  # æœªè¯†åˆ«åˆ°æƒ…æ„Ÿæ—¶ä¸ºNone,ä¸ä½¿ç”¨neutral
                            whisper_text = None


                        if Config.ONLY_REGISTERED_SPEAKERS and identity is None: continue
                        
                        # è®¡ç®—è¯­é€ŸæŒ‡æ ‡
                        duration_seconds = (end - start) / 1000.0
                        word_count = len(clean_text)  # ä¸­æ–‡æŒ‰å­—ç¬¦æ•°è®¡ç®—
                        speech_rate = word_count / duration_seconds if duration_seconds > 0 else 0
                        
                        # è®¡ç®—æ–‡æœ¬è´¨é‡
                        from collections import Counter
                        char_counts = Counter(clean_text)
                        most_common_char, most_common_count = char_counts.most_common(1)[0] if clean_text else ('', 0)
                        repeat_ratio = most_common_count / len(clean_text) if clean_text else 0
                        
                        filler_words = ['å—¯', 'å•Š', 'å‘ƒ', 'é¢', 'å“¦', 'å””']
                        text_no_punct = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼Ÿ,.!?]', '', clean_text)
                        filler_count = sum(text_no_punct.count(w) for w in filler_words) if text_no_punct else 0
                        filler_ratio = filler_count / len(text_no_punct) if text_no_punct else 0
                        noise_score = (repeat_ratio * 0.6 + filler_ratio * 0.4)
                        is_noise_flag = repeat_ratio > 0.4 or filler_ratio > 0.6
                        
                        text_quality = {
                            "is_noise": is_noise_flag,
                            "noise_score": round(noise_score, 3),
                            "repeat_ratio": round(repeat_ratio, 3),
                            "filler_ratio": round(filler_ratio, 3)
                        }
                        
                        # ç¡®å®šæƒ…æ„Ÿæ¥æº
                        emotion_source = "funasr"  # é»˜è®¤
                        original_emotion_tag = None
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹æƒ…æ„Ÿæ ‡ç­¾
                        for tag, emo_code in EMOTION_TAGS.items():
                            if tag.lower() in raw_text.lower():
                                original_emotion_tag = tag
                                break
                        
                        # å¦‚æœæœ‰ sensevoice_textï¼Œè¯´æ˜ä½¿ç”¨äº† SenseVoice
                        if sensevoice_text and emotion:
                            emotion_source = "sensevoice"
                            if not original_emotion_tag:
                                original_emotion_tag = f"<|{emotion}|>"
                        
                        processed_segments.append({
                            "text": clean_text, "start": start, "end": end,
                            "spk": identity or "Unknown", "emotion": emotion,
                            "whisper_text": whisper_text,
                            "sensevoice_text": sensevoice_text,
                            "confidence": float(f"{confidence:.3f}"),
                            "recognition_details": recognition_details,
                            "segment_audio_path": segment_audio_path,
                            
                            # è¯­é€ŸæŒ‡æ ‡
                            "speech_metrics": {
                                "duration_seconds": round(duration_seconds, 2),
                                "word_count": word_count,
                                "speech_rate": round(speech_rate, 2)
                            },
                            
                            # æ–‡æœ¬è´¨é‡è¯„ä¼°
                            "text_quality": text_quality,
                            
                            # æƒ…æ„Ÿè¯¦ç»†ä¿¡æ¯
                            "emotion_info": {
                                "emotion": emotion,
                                "source": emotion_source,
                                "original_tag": original_emotion_tag,
                                "detected_by_sensevoice": emotion_source == "sensevoice"
                            }
                        })
                    logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 3. é€æ®µå£°çº¹è¯†åˆ«] å®Œæˆã€‚")

                segments = processed_segments
                full_text = "".join([s["text"] for s in segments]) # Reconstruct from clean segments

            process_time = time.time() - request_start
            rtf = process_time / audio_duration if audio_duration > 0 else 0
            # RTF(Real-Time Factor)æ˜¯å®æ—¶å› å­ï¼Œè¯„ä¼°ç³»ç»Ÿå¤„ç†é€Ÿåº¦ä¸éŸ³é¢‘æ—¶é•¿çš„æ¯”ç‡
            # RTF < 1è¡¨ç¤ºå¯ä»¥å®æ—¶å¤„ç†ï¼ŒRTFè¶Šä½ç³»ç»Ÿæ€§èƒ½è¶Šå¥½
            logger.info(f"âœ… å®Œæˆ! éŸ³é¢‘:{audio_duration:.1f}s | è€—æ—¶:{process_time:.2f}s | RTF:{rtf:.3f} (RTF < 1è¡¨ç¤ºå¯å®æ—¶å¤„ç†ï¼Œå€¼è¶Šä½æ€§èƒ½è¶Šå¥½)")

            logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 4. ç»„è£…å“åº”] å¼€å§‹...")
            response_data = {
                "full_text": full_text,
                "segments": segments,
                "meta": {
                    "process_time": process_time,
                    "audio_duration": audio_duration,
                    "rtf": rtf,
                    "rtf_description": "Real-Time Factor(å®æ—¶å› å­)ï¼Œå¤„ç†æ—¶é—´/éŸ³é¢‘æ—¶é•¿ï¼ŒRTF < 1è¡¨ç¤ºå¯å®æ—¶å¤„ç†ï¼Œå€¼è¶Šä½æ€§èƒ½è¶Šå¥½"
                }
            }
            logger.info(f"ğŸ“¤  [ç”Ÿå‘½å‘¨æœŸ: 4. ç»„è£…å“åº”] å®Œæˆ, è¿”å› /transcribe ç»“æœ: {json.dumps(response_data, ensure_ascii=False, indent=2)}")
            
            # =================ã€ æ•°æ®åº“ä¿å­˜å’Œ LLM å¤„ç† ã€‘=================
            if processed_segments:
                try:
                    # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
                    summary = generate_conversation_summary(processed_segments, audio_duration)
                    
                    # è§£æå½•éŸ³æ—¶é—´
                    recording_time = parse_recording_time(file.filename)
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    success = save_to_db(file.filename, full_text, processed_segments, recording_time, summary)
                    
                    if success:
                        logger.info(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ (recording_time: {recording_time})")
                        if summary:
                            logger.info(f"  æ™ºèƒ½æ‘˜è¦: {summary['speaker_count']}ä½è¯´è¯äºº, {summary['total_segments']}ä¸ªåˆ†æ®µ")
                        
                        # æ·»åŠ åˆ° LLM æ‰¹é‡å¤„ç†é˜Ÿåˆ—
                        if LLMConfig.USE_GEMINI_LLM:
                            has_identified_speakers = any(seg.get('spk') != 'Unknown' for seg in processed_segments)
                            if (len(full_text) >= LLMConfig.LLM_MIN_TEXT_LENGTH and 
                                len(processed_segments) >= LLMConfig.LLM_MIN_SEGMENTS and 
                                has_identified_speakers):
                                add_to_llm_queue(file.filename, full_text, processed_segments)
                    else:
                        logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥")
                except Exception as e:
                    logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¼‚å¸¸: {e}")
                    logger.error(traceback.format_exc())
            # =========================================================

            return jsonify(response_data)

        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({"error": str(e)}), 500
        finally:
            for f in temp_files:
                if os.path.exists(f):
                    try: os.remove(f)
                    except: pass

@app.route("/speaker/<speaker_name>/sample/<sample_id>/audio")
def get_sample_audio(speaker_name, sample_id):
    """è·å–æŒ‡å®šè¯´è¯äººæ ·æœ¬çš„éŸ³é¢‘æ–‡ä»¶"""
    try:
        # é‡æ–°åŠ è½½å£°çº¹æ•°æ®åº“ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„
        load_speaker_db()
        if speaker_name not in speaker_db:
            return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
            
        speaker_data = speaker_db[speaker_name]
        if "samples" not in speaker_data:
            return jsonify({"error": f"No samples found for speaker '{speaker_name}'."}), 404
            
        # æŸ¥æ‰¾æŒ‡å®šæ ·æœ¬
        for sample in speaker_data["samples"]:
            if sample["id"] == sample_id:
                if "audio_path" in sample and os.path.exists(sample["audio_path"]):
                    return send_file(sample["audio_path"], as_attachment=True, download_name=sample["filename"])
                else:
                    return jsonify({"error": f"Audio file for sample '{sample_id}' not found."}), 404
        
        return jsonify({"error": f"Sample '{sample_id}' not found for speaker '{speaker_name}'."}), 404
    except Exception as e:
        logger.error(f"è·å–æ ·æœ¬éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to retrieve sample audio"}), 500


@app.route('/audio_segments/<path:filename>')
def serve_audio_segment(filename):
    """æä¾›éŸ³é¢‘ç‰‡æ®µé™æ€æ–‡ä»¶æœåŠ¡"""
    try:
        audio_segments_dir = os.path.join(FileMonitorConfig.SOURCE_DIR, 'audio_segments')
        return send_from_directory(audio_segments_dir, filename)
    except Exception as e:
        logger.error(f"è·å–éŸ³é¢‘ç‰‡æ®µå¤±è´¥: {str(e)}")
        return jsonify({"error": "Audio segment not found"}), 404


@app.route("/logs/stream")
def stream_logs():
    """SSE endpoint for real-time log streaming"""
    def generate_logs():
        # åˆ›å»ºä¸€ä¸ªæ–°çš„å®¢æˆ·ç«¯è¿æ¥
        client = type('Client', (), {'write': lambda self, msg: print(msg, end='', flush=True) or msg})
        
        # æ·»åŠ å®¢æˆ·ç«¯åˆ°SSEå¤„ç†å™¨
        sse_handler.add_client(client)
        try:
            # ä¿æŒè¿æ¥æ‰“å¼€
            while True:
                time.sleep(1)
        except GeneratorExit:
            # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥æ—¶ç§»é™¤å®¢æˆ·ç«¯
            sse_handler.remove_client(client)
    
    return Response(generate_logs(), mimetype='text/event-stream')

# =================== æ–‡ä»¶ç›‘æ§ ===================
def monitor_files():
    """ç›‘æ§æºç›®å½•ä¸­çš„æ–°éŸ³é¢‘æ–‡ä»¶å¹¶è‡ªåŠ¨è½¬å½•"""
    logger.info("ğŸ“‚ æ–‡ä»¶ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    logger.info(f"   ç›‘æ§ç›®å½•: {FileMonitorConfig.SOURCE_DIR}")
    logger.info(f"   æ‰«æé—´éš”: {FileMonitorConfig.SCAN_INTERVAL}ç§’")
    logger.info(f"   æ”¯æŒæ ¼å¼: {', '.join(FileMonitorConfig.SUPPORTED_FORMATS)}")
    
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    os.makedirs(FileMonitorConfig.SOURCE_DIR, exist_ok=True)
    processed_dir = os.path.join(FileMonitorConfig.SOURCE_DIR, FileMonitorConfig.PROCESSED_DIR)
    os.makedirs(processed_dir, exist_ok=True)
    failed_dir = os.path.join(FileMonitorConfig.SOURCE_DIR, FileMonitorConfig.FAILED_DIR)
    os.makedirs(failed_dir, exist_ok=True)
    
    processed_files = set()  # è®°å½•å·²å¤„ç†çš„æ–‡ä»¶ï¼Œé¿å…é‡å¤å¤„ç†
    
    while True:
        try:
            # æ‰«ææºç›®å½•
            logger.info(f"ğŸ” æ­£åœ¨æ‰«æ: {FileMonitorConfig.SOURCE_DIR}")
            if not os.path.exists(FileMonitorConfig.SOURCE_DIR):
                logger.warning(f"âš ï¸ æºç›®å½•ä¸å­˜åœ¨: {FileMonitorConfig.SOURCE_DIR}")
                time.sleep(FileMonitorConfig.SCAN_INTERVAL)
                continue
            
            files = []
            for filename in os.listdir(FileMonitorConfig.SOURCE_DIR):
                filepath = os.path.join(FileMonitorConfig.SOURCE_DIR, filename)
                
                # åªå¤„ç†æ–‡ä»¶ï¼Œè·³è¿‡ç›®å½•
                if not os.path.isfile(filepath):
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                ext = os.path.splitext(filename)[1].lower()
                if ext not in FileMonitorConfig.SUPPORTED_FORMATS:
                    continue
                
                # è·³è¿‡åŒ…å«TEMPçš„æ–‡ä»¶
                if 'TEMP' in filename or '_TEMP' in filename:
                    continue
                
                # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
                if filename in processed_files:
                    continue
                
                files.append((filename, filepath))
            
            # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿æ—¶é—´æˆ³æ—©çš„æ–‡ä»¶å…ˆå¤„ç†
            # æ–‡ä»¶åæ ¼å¼å¦‚: TermuxAudioRecording_2025-11-18_00-34-27.m4a
            # æˆ–: recording-20251115-131250.m4a
            files.sort(key=lambda x: x[0])  # æŒ‰æ–‡ä»¶åå­—æ¯é¡ºåºæ’åºï¼Œæ—¶é—´æˆ³æ—©çš„åœ¨å‰
            
            # å¤„ç†æ‰¾åˆ°çš„æ–‡ä»¶
            if files:
                logger.info(f"ğŸ” å‘ç° {len(files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
                logger.info(f"   å¤„ç†é¡ºåº: {files[0][0]} â†’ ... â†’ {files[-1][0]}")
                
                for filename, filepath in files:
                    try:
                        logger.info(f"ğŸ“¤ å¼€å§‹å¤„ç†: {filename}")
                        
                        # è°ƒç”¨æœ¬åœ°è½¬å½•API
                        with open(filepath, 'rb') as f:
                            files_data = {'audio_file': (filename, f, 'audio/mpeg')}
                            response = requests.post(
                                'http://localhost:5008/transcribes',
                                files=files_data,
                                timeout=7200  # 2å°æ—¶è¶…æ—¶
                            )
                        
                        if response.status_code == 200:
                            result = response.json()
                            logger.info(f"âœ… è½¬å½•å®Œæˆ: {filename}")
                            logger.info(f"   æ–‡æœ¬é•¿åº¦: {len(result.get('full_text', ''))} å­—")
                            logger.info(f"   åˆ†æ®µæ•°: {len(result.get('segments', []))}")
                            
                            # ç§»åŠ¨åˆ°å·²å¤„ç†ç›®å½•
                            processed_path = os.path.join(processed_dir, filename)
                            try:
                                shutil.move(filepath, processed_path)
                                logger.info(f"ğŸ“¦ å·²ç§»åŠ¨åˆ°: {FileMonitorConfig.PROCESSED_DIR}/{filename}")
                            except Exception as move_error:
                                logger.warning(f"âš ï¸ ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {move_error}")
                            
                            # æ ‡è®°ä¸ºå·²å¤„ç†
                            processed_files.add(filename)
                            
                        else:
                            logger.error(f"âŒ è½¬å½•å¤±è´¥: {filename} (HTTP {response.status_code})")
                            logger.error(f"   å“åº”: {response.text[:200]}")
                            
                            # ç§»åŠ¨åˆ°å¤±è´¥ç›®å½•
                            failed_path = os.path.join(failed_dir, filename)
                            try:
                                shutil.move(filepath, failed_path)
                                logger.info(f"ğŸš« å·²ç§»åŠ¨åˆ°å¤±è´¥ç›®å½•: {FileMonitorConfig.FAILED_DIR}/{filename}")
                            except Exception as move_error:
                                logger.warning(f"âš ï¸ ç§»åŠ¨åˆ°å¤±è´¥ç›®å½•å¤±è´¥: {move_error}")
                            
                    except requests.exceptions.Timeout:
                        logger.error(f"â±ï¸ è½¬å½•è¶…æ—¶: {filename}")
                        # è¶…æ—¶ä¹Ÿè®¤ä¸ºæ˜¯å¤±è´¥ï¼Œç§»åŠ¨åˆ°å¤±è´¥ç›®å½•
                        failed_path = os.path.join(failed_dir, filename)
                        try:
                            shutil.move(filepath, failed_path)
                            logger.info(f"ğŸš« è¶…æ—¶å·²ç§»åŠ¨åˆ°å¤±è´¥ç›®å½•: {FileMonitorConfig.FAILED_DIR}/{filename}")
                        except Exception as move_error:
                            logger.warning(f"âš ï¸ ç§»åŠ¨åˆ°å¤±è´¥ç›®å½•å¤±è´¥: {move_error}")
                            
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {filename}")
                        logger.error(f"   é”™è¯¯: {str(e)}")
                        logger.error(traceback.format_exc())
                        
                        # å…¶ä»–å¼‚å¸¸ä¹Ÿç§»åŠ¨åˆ°å¤±è´¥ç›®å½•
                        failed_path = os.path.join(failed_dir, filename)
                        try:
                            if os.path.exists(filepath):
                                shutil.move(filepath, failed_path)
                                logger.info(f"ğŸš« å‘ç”Ÿå¼‚å¸¸å·²ç§»åŠ¨åˆ°å¤±è´¥ç›®å½•: {FileMonitorConfig.FAILED_DIR}/{filename}")
                        except Exception as move_error:
                            logger.warning(f"âš ï¸ ç§»åŠ¨åˆ°å¤±è´¥ç›®å½•å¤±è´¥: {move_error}")
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡æ‰«æ
            time.sleep(FileMonitorConfig.SCAN_INTERVAL)
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶ç›‘æ§å¼‚å¸¸: {str(e)}")
            logger.error(traceback.format_exc())
            time.sleep(FileMonitorConfig.SCAN_INTERVAL)

# =================== å¯åŠ¨ ===================
def parse_args():
    parser = argparse.ArgumentParser(description='ASR Service')
    parser.add_argument('--source-path', type=str, help='Source directory for audio files')
    parser.add_argument('--port', type=int, help='Port to run the server on')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    
    # Update config from args
    if args.source_path:
        FileMonitorConfig.SOURCE_DIR = args.source_path
        print(f"é…ç½®æ›´æ–°: æºç›®å½• -> {args.source_path}")
        
    if args.port:
        Config.PORT = args.port
        print(f"é…ç½®æ›´æ–°: ç«¯å£ -> {args.port}")

    try:
        subprocess.run(["ffmpeg", "-version"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except FileNotFoundError:
        logger.critical("âŒ ç³»ç»Ÿæœªå®‰è£… FFmpegï¼")
        sys.exit(1)

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± å’Œè¡¨ç»“æ„
    print("åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
    if not init_pool():
        logger.critical("âŒ æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥ï¼")
        sys.exit(1)
    
    print("åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„...")
    if not init_db():
        logger.warning("âš ï¸ æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å¤±è´¥ï¼Œä½†æœåŠ¡å°†ç»§ç»­è¿è¡Œ")

    load_models()
    
    # å¯åŠ¨ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®šæ—¶ä»»åŠ¡
    cleanup_temp_dir()
    logger.info("ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨")
    
    # å¯åŠ¨æ–‡ä»¶ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_files, daemon=True)
    monitor_thread.start()
    logger.info("æ–‡ä»¶ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    print("ğŸ‰ æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print("ğŸ“Œ å£°çº¹æ³¨å†Œé¡µé¢: http://127.0.0.1:5008/register_page")
    print("ğŸ“Œ è¯­éŸ³è½¬å½•API: http://127.0.0.1:5008/transcribes (æœ¬åœ°ç›‘æ§ä¸“ç”¨)")
    print("ğŸ“Œ å¤–éƒ¨è°ƒç”¨API: http://127.0.0.1:5008/transcribe (ä¿ç•™ç»™NASä½¿ç”¨)")
    print(f"ğŸ“‚ æ–‡ä»¶ç›‘æ§ç›®å½•: {FileMonitorConfig.SOURCE_DIR}")
    print(f"â±ï¸  æ‰«æé—´éš”: {FileMonitorConfig.SCAN_INTERVAL}ç§’")
    print("ğŸ”§ APIä½¿ç”¨æ–¹æ³•: POSTè¯·æ±‚ï¼Œå‚æ•°å 'audio_file'ï¼Œä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
    print("ğŸ” ç¤ºä¾‹å‘½ä»¤: curl -X POST -F \"audio_file=@your_audio.wav\" http://127.0.0.1:5008/transcribes")
    app.run(host=Config.HOST, port=Config.PORT, debug=False, threaded=True)