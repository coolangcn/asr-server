#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, sys, logging, json, threading, subprocess, time, traceback, tempfile
import numpy as np
from scipy.spatial.distance import cosine
from flask import Flask, request, jsonify, render_template, send_file
from funasr import AutoModel  # ASR ç”¨ FunASR
from modelscope.pipelines import pipeline  # SV ç”¨ ModelScope
from modelscope.utils.constant import Tasks
import torch
import torchaudio
import shutil

# =================ã€ é…ç½® ã€‘=================
class Config:
    DEVICE = "cuda:0"
    HOST = '0.0.0.0'
    PORT = 5008
    SPEAKER_DB_FILE = "speaker_db_multi.json"
    
    ONLY_REGISTERED_SPEAKERS = True
    ASR_MODEL = "iic/SenseVoiceSmall"
    
    SV_MODELS = {
        "eres2net_large": {
            "id": "iic/speech_eres2net_large_200k_sv_zh-cn_16k-common",
            "rev": "v1.0.0",
            "threshold": 0.60,  # æé«˜é˜ˆå€¼
            "gap": 0.15         # å¢åŠ ç½®ä¿¡åº¦é—´éš”
        },
        "rdino_ecapa": {
            "id": "iic/speech_rdino_ecapa_tdnn_sv_zh-cn_cnceleb_16k",
            "rev": "v1.0.0",
            "threshold": 0.60,  # æé«˜é˜ˆå€¼
            "gap": 0.15         # å¢åŠ ç½®ä¿¡åº¦é—´éš”
        },
        "camplusplus": {
            "id": "iic/speech_campplus_sv_zh-cn_16k-common",
            "rev": "v1.0.0",
            "threshold": 0.60,  # æé«˜é˜ˆå€¼
            "gap": 0.15         # å¢åŠ ç½®ä¿¡åº¦é—´éš”
        }
    }
    
    MIN_SPEAKER_DURATION_MS = 800
    NORMALIZE_AUDIO = True
    DENOISE_AUDIO = True  # å¯ç”¨é«˜çº§é™å™ª
# ==========================================

EMOTION_TAGS = {
    "<|happy|>": "happy", "<|sad|>": "sad", "<|angry|>": "angry",
    "<|neutral|>": "neutral", "<|laughter|>": "laughter", "<|fearful|>": "fearful",
    "<|disgusted|>": "disgusted", "<|surprised|>": "surprised", "<|EMO_UNKNOWN|>": "neutral"
}
INVALID_TAGS = {"<|nospeech|>", "<|BGM|>", "<|Event_UNK|>", "<|music|>"}

# æ–°å¢ï¼šå®šä¹‰è¯´è¯äººæ•°æ®ç»“æ„
# {
#   "speaker_name": {
#     "samples": [
#       {
#         "id": "sample_id",
#         "filename": "file_name.wav",
#         "timestamp": "2023-01-01 12:00:00",
#         "embeddings": {
#           "eres2net_large": [...],
#           "rdino_ecapa": [...]
#         }
#       }
#     ],
#     "avg_embeddings": {
#       "eres2net_large": [...],
#       "rdino_ecapa": [...]
#     }
#   }
# }

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger()
app = Flask(__name__)
logging.getLogger('werkzeug').setLevel(logging.ERROR)

asr_pipeline = None
sv_pipelines = {}
speaker_db = {}
gpu_lock = threading.Lock()
db_lock = threading.Lock()

# =================== æ¨¡å‹åŠ è½½ ===================
def load_models():
    global asr_pipeline, sv_pipelines
    print("\n====== ğŸš€ å¯åŠ¨ SOTA èåˆæœåŠ¡ ======")
    
    load_speaker_db()

    # 2. åŠ è½½ ASR (FunASR)
    print(f"ğŸ§  åŠ è½½ ASR: {Config.ASR_MODEL} ...")
    asr_pipeline = AutoModel(model=Config.ASR_MODEL, trust_remote_code=True, device=Config.DEVICE, disable_update=True)

    # 3. åŠ è½½ SV æ¨¡å‹
    for name, conf in Config.SV_MODELS.items():
        print(f"ğŸ” åŠ è½½ SV [{name}] : {conf['id']} ...")
        sv_pipelines[name] = pipeline(
            task=Tasks.speaker_verification,
            model=conf['id'], 
            model_revision=conf['rev'], 
            device=Config.DEVICE.split(':')[0]
        )
    print(f"âœ… æœåŠ¡å°±ç»ª | ASR: SenseVoice | SV: {list(sv_pipelines.keys())}\n")

def load_speaker_db():
    global speaker_db
    with db_lock:
        if os.path.exists(Config.SPEAKER_DB_FILE):
            try:
                with open(Config.SPEAKER_DB_FILE, 'r', encoding='utf-8') as f:
                    loaded_db = json.load(f)
                
                # å…¼å®¹æ—§æ•°æ®ç»“æ„
                converted_db = {}
                for name, data in loaded_db.items():
                    if "samples" in data and "avg_embeddings" in data:
                        # æ–°æ•°æ®ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨
                        converted_db[name] = data
                    else:
                        # æ—§æ•°æ®ç»“æ„ï¼Œè½¬æ¢ä¸ºæ–°ç»“æ„
                        logger.info(f"ğŸ”„ è½¬æ¢æ—§æ•°æ®ç»“æ„ for speaker: {name}")
                        converted_db[name] = {
                            "samples": [],  # æ—§æ•°æ®ç»“æ„æ²¡æœ‰æ ·æœ¬ä¿¡æ¯
                            "avg_embeddings": data  # æ—§æ•°æ®ç»“æ„ç›´æ¥æ˜¯åµŒå…¥å­—å…¸
                        }
                
                speaker_db = converted_db
                logger.info(f"ğŸ“š å£°çº¹åº“å·²æŒ‚è½½: {len(speaker_db)} äºº")
            except Exception as e:
                logger.error(f"å£°çº¹åº“æŸå: {e}")
                speaker_db = {}
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {Config.SPEAKER_DB_FILE}ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®åº“ã€‚")
            speaker_db = {}

# =================== éŸ³é¢‘é¢„å¤„ç† ===================
def preprocess_audio(input_path, output_path):
    # å¦‚æœå¯ç”¨äº†é«˜çº§é™å™ªï¼Œå…ˆè¿›è¡Œé™å™ªå¤„ç†
    if Config.DENOISE_AUDIO:
        denoised_path = input_path + ".denoised.wav"
        if advanced_denoise(input_path, denoised_path):
            input_path = denoised_path
        else:
            logger.warning("é«˜çº§é™å™ªå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘")
    
    cmd = ["ffmpeg", "-v", "error", "-y", "-i", input_path]
    filters = ["loudnorm=I=-14:TP=-1.5:LRA=11"] if Config.NORMALIZE_AUDIO else []
    if filters: cmd.extend(["-af", ",".join(filters)])
    cmd.extend(["-ac", "1", "-ar", "16000", output_path])
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL)
        # æ¸…ç†ä¸´æ—¶é™å™ªæ–‡ä»¶
        if Config.DENOISE_AUDIO and input_path.endswith(".denoised.wav"):
            try:
                os.remove(input_path)
            except:
                pass
        return True
    except Exception as e:
        logger.error(f"FFmpeg é¢„å¤„ç†å¤±è´¥: {e}")
        return False

def advanced_denoise(input_path, output_path):
    """ä½¿ç”¨è°±å‡æ³•è¿›è¡Œé«˜çº§é™å™ª"""
    try:
        # åŠ è½½éŸ³é¢‘
        waveform, sample_rate = torchaudio.load(input_path)
        
        # å¦‚æœé‡‡æ ·ç‡ä¸æ˜¯16kHzï¼Œå…ˆè¿›è¡Œé‡é‡‡æ ·
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resampler(waveform)
            sample_rate = 16000
        
        # è½¬æ¢ä¸ºå•å£°é“
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # ç®€åŒ–çš„è°±å‡æ³•é™å™ª
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        audio_np = waveform.numpy()[0]
        
        # è®¡ç®—çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
        from scipy import signal
        frequencies, times, Zxx = signal.stft(audio_np, fs=sample_rate, nperseg=512)
        
        # ä¼°è®¡å™ªå£°è°±ï¼ˆå‡è®¾å‰100msä¸ºå™ªå£°ï¼‰
        noise_seg_len = min(int(0.1 * sample_rate), len(audio_np))
        noise_segment = audio_np[:noise_seg_len]
        _, _, noise_stft = signal.stft(noise_segment, fs=sample_rate, nperseg=512)
        noise_spectrum = np.mean(np.abs(noise_stft), axis=1)
        
        # åº”ç”¨è°±å‡æ³•
        magnitude = np.abs(Zxx)
        phase = np.angle(Zxx)
        
        # å‡å»å™ªå£°è°±çš„ä¼°è®¡å€¼
        noise_factor = 1.5
        magnitude_denoised = np.maximum(magnitude - noise_factor * noise_spectrum[:, np.newaxis], 0)
        
        # é‡æ„ä¿¡å·
        Zxx_denoised = magnitude_denoised * np.exp(1j * phase)
        _, audio_denoised = signal.istft(Zxx_denoised, fs=sample_rate)
        
        # è£å‰ªåˆ°åŸå§‹é•¿åº¦
        audio_denoised = audio_denoised[:len(audio_np)]
        
        # ä¿å­˜é™å™ªåçš„éŸ³é¢‘
        waveform_denoised = torch.tensor(audio_denoised).unsqueeze(0)
        torchaudio.save(output_path, waveform_denoised, sample_rate)
        
        return True
    except Exception as e:
        logger.error(f"é«˜çº§é™å™ªå¤„ç†å¤±è´¥: {e}")
        return False

def extract_segment(source_path, start_ms, end_ms, output_path):
    if start_ms >= end_ms: return False
    start_sec = start_ms / 1000.0
    duration = (end_ms - start_ms) / 1000.0
    cmd = ["ffmpeg", "-v", "error", "-y", "-ss", f"{start_sec:.3f}", "-t", f"{duration:.3f}", "-i", source_path, "-ac", "1", "-ar", "16000", output_path]
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL)
        return True
    except:
        return False

# =================== æå– embedding ===================
def extract_embedding_from_file(sv_pipe, wav_path):
    try:
        model = sv_pipe.model
        audio, sr = torchaudio.load(wav_path)
        if sr != 16000:
            resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
            audio = resample(audio)
        
        audio = audio.mean(dim=0, keepdim=True) # [C, T] -> [1, T]

        with torch.no_grad():
            out = model(audio)
            if isinstance(out, dict):
                emb = out.get("spk_embedding")
            else:
                emb = out
        return emb.squeeze().cpu().numpy()

    except Exception as e:
        logger.error(f"âŒ extract_embedding å¤±è´¥: {e}")
        return None

# =================== å¤šæ¨¡å‹äº¤å‰éªŒè¯ ===================
def identify_speaker_fusion(segment_path):
    if not speaker_db: 
        logger.info("ğŸ¤·â€â™‚ï¸ å£°çº¹æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè¯†åˆ«")
        return None, 0.0, []

    model_votes = {}
    model_scores = {}

    logger.info(f"ğŸ¯ å¼€å§‹å£°çº¹è¯†åˆ«: éŸ³é¢‘æ®µè·¯å¾„={segment_path}")
    logger.info(f"ğŸ“‹ å£°çº¹æ•°æ®åº“åŒ…å« {len(speaker_db)} ä¸ªè¯´è¯äºº")

    for model_name, sv_pipe in sv_pipelines.items():
        logger.info(f"ğŸ” å¼€å§‹ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        emb_a = extract_embedding_from_file(sv_pipe, segment_path)
        if emb_a is None:
            logger.error(f"âŒ æ¨¡å‹ {model_name} ç‰¹å¾æå–å¤±è´¥")
            model_votes[model_name] = "Failed"
            continue

        scores = []
        conf = Config.SV_MODELS[model_name]
        threshold = conf['threshold']
        gap = conf['gap']
        logger.info(f"ğŸ“Œ æ¨¡å‹ {model_name} é˜ˆå€¼: {threshold}, ç½®ä¿¡åº¦é—´éš”: {gap}")

        for name, speaker_data in speaker_db.items():
            # ä½¿ç”¨å¹³å‡åµŒå…¥è¿›è¡Œæ¯”è¾ƒ
            if "avg_embeddings" not in speaker_data or model_name not in speaker_data["avg_embeddings"]: 
                continue
            emb_b = np.array(speaker_data["avg_embeddings"][model_name]).flatten()
            score = 1 - cosine(emb_a.flatten(), emb_b)
            scores.append((name, score))
            logger.info(f"ğŸ’¯ æ¨¡å‹ {model_name} ä¸è¯´è¯äºº {name} çš„ç›¸ä¼¼åº¦: {score:.6f}")

        if not scores:
            logger.warning(f"âš ï¸ æ¨¡å‹ {model_name} æœªæ‰¾åˆ°åŒ¹é…çš„è¯´è¯äººæ•°æ®")
            model_votes[model_name] = "NoDB"
            continue

        scores.sort(key=lambda x: x[1], reverse=True)
        top1_name, top1_score = scores[0]
        top2_name, top2_score = scores[1] if len(scores) > 1 else (None, 0.0)
        score_gap = top1_score - top2_score
        
        logger.info(f"ğŸ† æ¨¡å‹ {model_name} è¯†åˆ«ç»“æœ: ç¬¬ä¸€å {top1_name} (å¾—åˆ†: {top1_score:.6f}), ç¬¬äºŒå {top2_name} (å¾—åˆ†: {top2_score:.6f}), å·®è·: {score_gap:.6f}")

        if top1_score >= threshold and score_gap >= gap:
            model_votes[model_name] = top1_name
            model_scores[model_name] = top1_score
            logger.info(f"âœ… æ¨¡å‹ {model_name} éªŒè¯é€šè¿‡: {top1_name} (å¾—åˆ†: {top1_score:.6f} â‰¥ é˜ˆå€¼ {threshold})")
        else:
            model_votes[model_name] = "Unknown"
            model_scores[model_name] = top1_score
            reason = []
            if top1_score < threshold:
                reason.append(f"å¾—åˆ† {top1_score:.6f} < é˜ˆå€¼ {threshold}")
            if score_gap < gap:
                reason.append(f"å·®è· {score_gap:.6f} < ç½®ä¿¡åº¦é—´éš” {gap}")
            logger.info(f"âŒ æ¨¡å‹ {model_name} éªŒè¯å¤±è´¥: {', '.join(reason)}")

    logger.info(f"ğŸ“Š å¤šæ¨¡å‹æŠ•ç¥¨ç»“æœ: {model_votes}")
    votes = list(model_votes.values())
    first_vote = votes[0] if votes else ""
    
    if first_vote not in ["Unknown", "Failed", "NoDB"] and all(v == first_vote for v in votes):
        avg_confidence = np.mean(list(model_scores.values()))
        logger.info(f"ğŸ‰ äº¤å‰éªŒè¯æˆåŠŸ: [{first_vote}] | å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f} | æ‰€æœ‰æ¨¡å‹ä¸€è‡´åŒæ„")
        # ç”Ÿæˆè¯†åˆ«è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
        recognition_details = []
        for model_name, result in model_votes.items():
            if result in ["Unknown", "Failed", "NoDB"]:
                recognition_details.append(f"æ¨¡å‹ {model_name}: {result}")
            else:
                recognition_details.append(f"æ¨¡å‹ {model_name}: è¯†åˆ«ä¸º {result} (ç›¸ä¼¼åº¦: {model_scores[model_name]:.6f})")
        recognition_details.append(f"æœ€ç»ˆè¯†åˆ«ç»“æœ: {first_vote} (å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f})")
        
        return first_vote, avg_confidence, recognition_details
    else:
        # ç”Ÿæˆè¯†åˆ«å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯
        recognition_details = []
        for model_name, result in model_votes.items():
            if result in ["Unknown", "Failed", "NoDB"]:
                recognition_details.append(f"æ¨¡å‹ {model_name}: {result}")
        recognition_details.append(f"æœ€ç»ˆè¯†åˆ«ç»“æœ: è¯†åˆ«å¤±è´¥ï¼Œæ¨¡å‹æŠ•ç¥¨ä¸ä¸€è‡´æˆ–è¯†åˆ«å¤±è´¥")
        
        logger.info(f"âŒ äº¤å‰éªŒè¯å¤±è´¥: æ¨¡å‹æŠ•ç¥¨ä¸ä¸€è‡´æˆ–è¯†åˆ«å¤±è´¥")
        return None, 0.0, []

# =================== Flask æ¥å£ ===================
@app.route("/")
def home():
    return render_template("register.html")

@app.route("/register_page")
def register_page():
    return render_template("register.html")

@app.route("/manage")
def manage_page():
    return render_template("manage.html")

@app.route("/speakers", methods=["GET"])
def get_speakers():
    """è·å–æ‰€æœ‰è¯´è¯äººåˆ—è¡¨"""
    try:
        # é‡æ–°åŠ è½½å£°çº¹æ•°æ®åº“ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„
        load_speaker_db()
        # è¿”å›è¯´è¯äººåˆ—è¡¨ï¼ˆä¸åŒ…å«å…·ä½“çš„embeddingæ•°æ®ï¼‰
        speakers_summary = {}
        for name, data in speaker_db.items():
            sample_count = len(data.get("samples", []))
            model_names = list(data.get("avg_embeddings", {}).keys())
            speakers_summary[name] = {
                "sample_count": sample_count,
                "models": model_names
            }
        return jsonify({"speakers": speakers_summary})
    except Exception as e:
        logger.error(f"è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to retrieve speakers"}), 500

@app.route("/speaker/<speaker_name>", methods=["GET"])
def get_speaker_samples(speaker_name):
    """è·å–æŒ‡å®šè¯´è¯äººçš„æ ·æœ¬åˆ—è¡¨"""
    try:
        # é‡æ–°åŠ è½½å£°çº¹æ•°æ®åº“ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„
        load_speaker_db()
        if speaker_name not in speaker_db:
            return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
            
        speaker_data = speaker_db[speaker_name]
        # è¿”å›æ ·æœ¬ä¿¡æ¯ï¼ˆä¸åŒ…å«å…·ä½“çš„embeddingæ•°æ®ï¼‰
        samples_info = []
        for sample in speaker_data.get("samples", []):
            samples_info.append({
                "id": sample["id"],
                "filename": sample["filename"],
                "timestamp": sample["timestamp"]
            })
        
        return jsonify({
            "speaker_name": speaker_name,
            "sample_count": len(samples_info),
            "samples": samples_info,
            "models": list(speaker_data.get("avg_embeddings", {}).keys())
        })
    except Exception as e:
        logger.error(f"è·å–è¯´è¯äººæ ·æœ¬åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to retrieve speaker samples"}), 500

@app.route("/speaker/<speaker_name>", methods=["DELETE"])
def delete_speaker(speaker_name):
    """åˆ é™¤æŒ‡å®šè¯´è¯äºº"""
    try:
        with db_lock:
            if speaker_name in speaker_db:
                del speaker_db[speaker_name]
                # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
                with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(speaker_db, f, indent=2, ensure_ascii=False)
                logger.info(f"âœ… æˆåŠŸåˆ é™¤è¯´è¯äºº: {speaker_name}")
                return jsonify({"message": f"Speaker '{speaker_name}' deleted successfully."})
            else:
                return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
    except Exception as e:
        logger.error(f"åˆ é™¤è¯´è¯äººå¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to delete speaker"}), 500

@app.route("/speaker/<speaker_name>/sample/<sample_id>", methods=["DELETE"])
def delete_speaker_sample(speaker_name, sample_id):
    """åˆ é™¤æŒ‡å®šè¯´è¯äººçš„ç‰¹å®šæ ·æœ¬"""
    try:
        with db_lock:
            if speaker_name not in speaker_db:
                return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
                
            speaker_data = speaker_db[speaker_name]
            if "samples" not in speaker_data:
                return jsonify({"error": f"No samples found for speaker '{speaker_name}'."}), 404
                
            # æŸ¥æ‰¾å¹¶åˆ é™¤æŒ‡å®šæ ·æœ¬
            samples = speaker_data["samples"]
            sample_to_remove = None
            sample_index = -1
            for i, sample in enumerate(samples):
                if sample["id"] == sample_id:
                    sample_to_remove = sample
                    sample_index = i
                    break
                    
            if sample_to_remove is None:
                return jsonify({"error": f"Sample '{sample_id}' not found for speaker '{speaker_name}'."}), 404
                
            # åˆ é™¤æ ·æœ¬çš„éŸ³é¢‘æ–‡ä»¶
            if "audio_path" in sample_to_remove and os.path.exists(sample_to_remove["audio_path"]):
                try:
                    os.remove(sample_to_remove["audio_path"])
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†éŸ³é¢‘æ–‡ä»¶: {sample_to_remove['audio_path']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ é™¤éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {sample_to_remove['audio_path']}, é”™è¯¯: {str(e)}")
            
            # ä»æ•°æ®åº“ä¸­ç§»é™¤æ ·æœ¬è®°å½•
            del samples[sample_index]
                
            # å¦‚æœåˆ é™¤æ ·æœ¬åæ²¡æœ‰å‰©ä½™æ ·æœ¬ï¼Œåˆ™åˆ é™¤æ•´ä¸ªè¯´è¯äºº
            if len(samples) == 0:
                del speaker_db[speaker_name]
                # åˆ é™¤è¯´è¯äººçš„ç›®å½•
                speaker_dir = os.path.join("speaker_samples", speaker_name)
                if os.path.exists(speaker_dir):
                    try:
                        shutil.rmtree(speaker_dir)
                        logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†è¯´è¯äººç›®å½•: {speaker_dir}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤è¯´è¯äººç›®å½•å¤±è´¥: {speaker_dir}, é”™è¯¯: {str(e)}")
                
                with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(speaker_db, f, indent=2, ensure_ascii=False)
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†è¯´è¯äºº {speaker_name}ï¼ˆæœ€åä¸€ä¸ªæ ·æœ¬å·²åˆ é™¤ï¼‰")
                return jsonify({"message": f"Speaker '{speaker_name}' deleted (last sample removed)."})

            # é‡æ–°è®¡ç®—å¹³å‡åµŒå…¥
            all_model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}
            for sample in samples:
                for model_name, emb in sample["embeddings"].items():
                    all_model_embeddings[model_name].append(np.array(emb))
            
            # è®¡ç®—æ–°çš„å¹³å‡åµŒå…¥
            new_avg_embeddings = {}
            for model_name, emb_list in all_model_embeddings.items():
                if emb_list:
                    avg_emb = np.mean(emb_list, axis=0)
                    new_avg_embeddings[model_name] = avg_emb.tolist()
            
            speaker_db[speaker_name]["avg_embeddings"] = new_avg_embeddings
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
            with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                json.dump(speaker_db, f, indent=2, ensure_ascii=False)
                
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº†è¯´è¯äºº {speaker_name} çš„æ ·æœ¬ {sample_id}")
            return jsonify({
                "message": f"Sample '{sample_id}' deleted from speaker '{speaker_name}'.",
                "remaining_samples": len(samples)
            })
    except Exception as e:
        logger.error(f"åˆ é™¤è¯´è¯äººæ ·æœ¬å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to delete speaker sample"}), 500

@app.route("/register", methods=["POST"])
def register_speaker():
    temp_files = []
    with gpu_lock:
        try:
            if 'speaker_name' not in request.form or not request.form['speaker_name']:
                return jsonify({"error": "Speaker name is required"}), 400
            
            speaker_name = request.form['speaker_name']
            audio_files = request.files.getlist('audio_files')
            
            # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦å¢å¼ºæ¨¡å¼
            enhance_mode = speaker_name in speaker_db

            if not audio_files:
                return jsonify({"error": "At least one audio file is required"}), 400

            action = "å¢å¼º" if enhance_mode else "æ³¨å†Œ"
            logger.info(f"ğŸ“¥ å¼€å§‹{action}æ–°å£°çº¹: {speaker_name} | æ–‡ä»¶æ•°: {len(audio_files)}")
            
            # åˆ›å»ºè¯´è¯äººæ ·æœ¬ç›®å½•
            speaker_dir = os.path.join("speaker_samples", speaker_name)
            if not os.path.exists(speaker_dir):
                os.makedirs(speaker_dir)
            
            # æ”¶é›†æ–°æ ·æœ¬æ•°æ®
            new_samples = []
            model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}

            for file in audio_files:
                raw_temp = os.path.join(tempfile.gettempdir(), f"reg_raw_{int(time.time())}_{file.filename}")
                file.save(raw_temp)
                temp_files.append(raw_temp)
                
                proc_temp = os.path.join(tempfile.gettempdir(), f"reg_proc_{int(time.time())}.wav")
                temp_files.append(proc_temp)

                if not preprocess_audio(raw_temp, proc_temp):
                    logger.warning(f"âš ï¸ æ–‡ä»¶ {file.filename} é¢„å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
                    continue

                # ä¸ºæ¯ä¸ªæ¨¡å‹æå–åµŒå…¥
                sample_embeddings = {}
                for model_name, sv_pipe in sv_pipelines.items():
                    emb = extract_embedding_from_file(sv_pipe, proc_temp)
                    if emb is not None:
                        sample_embeddings[model_name] = emb.tolist()
                        model_embeddings[model_name].append(emb)
                    else:
                        logger.warning(f"âš ï¸ ä» {file.filename} æå– {model_name} embedding å¤±è´¥ã€‚")

                # ä¿å­˜æ ·æœ¬ä¿¡æ¯å’ŒéŸ³é¢‘æ–‡ä»¶
                if sample_embeddings:  # åªæœ‰å½“è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹æˆåŠŸæå–åµŒå…¥æ—¶æ‰ä¿å­˜æ ·æœ¬
                    # ç”Ÿæˆå”¯ä¸€çš„æ ·æœ¬ID
                    sample_id = f"{int(time.time())}_{hash(file.filename) % 10000}"
                    
                    # ä¿å­˜å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶
                    sample_audio_path = os.path.join(speaker_dir, f"{sample_id}.wav")
                    shutil.copy2(proc_temp, sample_audio_path)
                    
                    sample_info = {
                        "id": sample_id,
                        "filename": file.filename,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "audio_path": sample_audio_path,
                        "embeddings": sample_embeddings
                    }
                    new_samples.append(sample_info)

            # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡åµŒå…¥
            avg_embeddings = {}
            for model_name, emb_list in model_embeddings.items():
                if not emb_list:
                    continue
                avg_emb = np.mean(emb_list, axis=0)
                avg_embeddings[model_name] = avg_emb.tolist()
                logger.info(f"  - æ¨¡å‹ [{model_name}] å¤„ç†äº† {len(emb_list)} ä¸ªæ ·æœ¬")

            if not avg_embeddings:
                return jsonify({"error": "Failed to extract embeddings from any samples"}), 500

            with db_lock:
                # å¦‚æœè¯´è¯äººå·²å­˜åœ¨ï¼Œåˆ™æ·»åŠ æ–°æ ·æœ¬å¹¶æ›´æ–°å¹³å‡åµŒå…¥
                if enhance_mode and speaker_name in speaker_db:
                    # æ·»åŠ æ–°æ ·æœ¬åˆ°ç°æœ‰æ ·æœ¬åˆ—è¡¨
                    if "samples" not in speaker_db[speaker_name]:
                        speaker_db[speaker_name]["samples"] = []
                    speaker_db[speaker_name]["samples"].extend(new_samples)
                    
                    # é‡æ–°è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡åµŒå…¥
                    all_model_embeddings = {model_name: [] for model_name in sv_pipelines.keys()}
                    
                    # æ·»åŠ ç°æœ‰æ ·æœ¬çš„åµŒå…¥
                    for sample in speaker_db[speaker_name]["samples"]:
                        for model_name, emb in sample["embeddings"].items():
                            all_model_embeddings[model_name].append(np.array(emb))
                    
                    # é‡æ–°è®¡ç®—å¹³å‡åµŒå…¥
                    new_avg_embeddings = {}
                    for model_name, emb_list in all_model_embeddings.items():
                        if emb_list:
                            avg_emb = np.mean(emb_list, axis=0)
                            new_avg_embeddings[model_name] = avg_emb.tolist()
                    
                    speaker_db[speaker_name]["avg_embeddings"] = new_avg_embeddings
                    logger.info(f"ğŸ”„ å¢å¼ºäº†è¯´è¯äºº {speaker_name} çš„å£°çº¹ï¼Œæ–°å¢ {len(new_samples)} ä¸ªæ ·æœ¬")
                else:
                    # åˆ›å»ºæ–°çš„è¯´è¯äººæ¡ç›®
                    speaker_db[speaker_name] = {
                        "samples": new_samples,
                        "avg_embeddings": avg_embeddings
                    }
                    logger.info(f"ğŸ†• åˆ›å»ºäº†æ–°è¯´è¯äºº {speaker_name} çš„å£°çº¹ï¼ŒåŒ…å« {len(new_samples)} ä¸ªæ ·æœ¬")
                    
                # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
                with open(Config.SPEAKER_DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(speaker_db, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… å£°çº¹{action}æˆåŠŸ: {speaker_name}")
            return jsonify({
                "message": f"Speaker '{speaker_name}' {action} successfully.",
                "samples_added": len(new_samples)
            })

        except Exception as e:
            logger.error(f"âŒ æ³¨å†Œå¼‚å¸¸: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({"error": "An internal error occurred during registration."} ), 500
        finally:
            for f in temp_files:
                if os.path.exists(f):
                    try: os.remove(f)
                    except: pass

@app.route("/transcribe", methods=["POST"])
def transcribe_audio():
    request_start = time.time()
    temp_files = []

    with gpu_lock:
        try:
            if 'audio_file' not in request.files: return jsonify({"error": "No file uploaded"}), 400
            
            file = request.files['audio_file']
            raw_temp = os.path.join(tempfile.gettempdir(), f"raw_{int(time.time())}_{file.filename}")
            file.save(raw_temp)
            temp_files.append(raw_temp)
            proc_temp = os.path.join(tempfile.gettempdir(), f"proc_{int(time.time())}.wav")
            temp_files.append(proc_temp)
            
            logger.info(f"ğŸ“¥ æ”¶åˆ°è½¬å½•ä»»åŠ¡: {file.filename}")
            if not preprocess_audio(raw_temp, proc_temp):
                return jsonify({"error": "Audio preprocessing failed"}), 500
            
            audio_duration = 0
            try:
                probe = subprocess.check_output(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', proc_temp])
                audio_duration = float(probe)
            except: pass

            res = asr_pipeline.generate(input=proc_temp, language="auto", use_itn=True)
            full_text = ""
            segments = []

            if res and isinstance(res, list) and len(res) > 0:
                item = res[0]
                full_text = item.get("text", "")
                raw_segments = item.get("sentence_info", [{"text": full_text, "start": 0, "end": int(audio_duration * 1000)}])

                processed_segments = []
                for seg in raw_segments:
                    raw_text = seg.get("text", "")
                    start, end = seg.get("start", 0), seg.get("end", 0)
                    if any(tag in raw_text for tag in INVALID_TAGS): continue

                    emotion = next((emo_code for tag, emo_code in EMOTION_TAGS.items() if tag in raw_text), "neutral")

                    clean_text = raw_text
                    for tag in (list(EMOTION_TAGS.keys()) + list(INVALID_TAGS) + ["<|zh|>", "<|en|>", "<|yue|>", "<|withitn|>", "<|speech|>"]):
                        clean_text = clean_text.replace(tag, "")
                    clean_text = clean_text.strip()
                    if not clean_text: continue

                    identity, confidence = None, 0.0
                    if (end - start) > Config.MIN_SPEAKER_DURATION_MS:
                        seg_wav = os.path.join(tempfile.gettempdir(), f"seg_{start}_{int(time.time())}.wav")
                        if extract_segment(proc_temp, start, end, seg_wav):
                            temp_files.append(seg_wav)
                            identity, confidence, recognition_details = identify_speaker_fusion(seg_wav)

                    if Config.ONLY_REGISTERED_SPEAKERS and identity is None: continue
                    
                    processed_segments.append({
                        "text": clean_text, "start": start, "end": end,
                        "spk": identity or "Unknown", "emotion": emotion,
                        "confidence": float(f"{confidence:.3f}"),
                        "recognition_details": recognition_details
                    })

                segments = processed_segments
                if Config.ONLY_REGISTERED_SPEAKERS:
                    full_text = "".join([s["text"] for s in segments])

            process_time = time.time() - request_start
            rtf = process_time / audio_duration if audio_duration > 0 else 0
            logger.info(f"âœ… å®Œæˆ! éŸ³é¢‘:{audio_duration:.1f}s | è€—æ—¶:{process_time:.2f}s | RTF:{rtf:.3f}")

            return jsonify({"full_text": full_text, "segments": segments, "meta": {"process_time": process_time, "audio_duration": audio_duration, "rtf": rtf}})

        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({"error": str(e)}), 500
        finally:
            for f in temp_files:
                if os.path.exists(f):
                    try: os.remove(f)
                    except: pass

@app.route("/speaker/<speaker_name>/sample/<sample_id>/audio")
def get_sample_audio(speaker_name, sample_id):
    """è·å–æŒ‡å®šè¯´è¯äººæ ·æœ¬çš„éŸ³é¢‘æ–‡ä»¶"""
    try:
        # é‡æ–°åŠ è½½å£°çº¹æ•°æ®åº“ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„
        load_speaker_db()
        if speaker_name not in speaker_db:
            return jsonify({"error": f"Speaker '{speaker_name}' not found."}), 404
            
        speaker_data = speaker_db[speaker_name]
        if "samples" not in speaker_data:
            return jsonify({"error": f"No samples found for speaker '{speaker_name}'."}), 404
            
        # æŸ¥æ‰¾æŒ‡å®šæ ·æœ¬
        for sample in speaker_data["samples"]:
            if sample["id"] == sample_id:
                if "audio_path" in sample and os.path.exists(sample["audio_path"]):
                    return send_file(sample["audio_path"], as_attachment=True, download_name=sample["filename"])
                else:
                    return jsonify({"error": f"Audio file for sample '{sample_id}' not found."}), 404
        
        return jsonify({"error": f"Sample '{sample_id}' not found for speaker '{speaker_name}'."}), 404
    except Exception as e:
        logger.error(f"è·å–æ ·æœ¬éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({"error": "Failed to retrieve sample audio"}), 500

# =================== å¯åŠ¨ ===================
if __name__ == "__main__":
    try:
        subprocess.run(["ffmpeg", "-version"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except FileNotFoundError:
        logger.critical("âŒ ç³»ç»Ÿæœªå®‰è£… FFmpegï¼")
        sys.exit(1)

    load_models()
    print("ğŸ‰ æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print("ğŸ“Œ å£°çº¹æ³¨å†Œé¡µé¢: http://127.0.0.1:5008/register_page")
    print("ğŸ“Œ è¯­éŸ³è½¬å½•API: http://127.0.0.1:5008/transcribe")
    print("ğŸ”§ APIä½¿ç”¨æ–¹æ³•: POSTè¯·æ±‚ï¼Œå‚æ•°å 'audio_file'ï¼Œä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
    print("ğŸ” ç¤ºä¾‹å‘½ä»¤: curl -X POST -F \"audio_file=@your_audio.wav\" http://127.0.0.1:5008/transcribe")
    app.run(host=Config.HOST, port=Config.PORT, debug=False, threaded=True)