#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ASRæœåŠ¡ - æ•´åˆæ–‡ä»¶ç›‘æ§å’Œè½¬å½•åŠŸèƒ½

è¿™ä¸ªè„šæœ¬æ•´åˆäº†ï¼š
1. ASRæœåŠ¡ç«¯ï¼ˆasr_server.pyï¼‰- æä¾›è½¬å½•APIå’Œæ¨¡å‹åŠ è½½
2. æ–‡ä»¶ç›‘æ§ï¼ˆtranscribe.pyï¼‰- è‡ªåŠ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶

ä¼˜åŠ¿ï¼š
- åœ¨åŒä¸€è¿›ç¨‹ä¸­è¿è¡Œï¼Œç›´æ¥å‡½æ•°è°ƒç”¨ï¼ˆæ— HTTPå¼€é”€ï¼‰
- å…±äº«AIæ¨¡å‹å†…å­˜
- ç®€åŒ–éƒ¨ç½²
"""

import os
import sys
import time
import threading
import subprocess
import shutil
import re
import json
import traceback
from datetime import datetime

# å¯¼å…¥ASRæœåŠ¡æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
import asr_server
from asr_server import Config as ASRConfig, logger, preprocess_audio
from db_manager import init_pool, init_db, save_to_db, close_pool

# =================ã€æ–‡ä»¶ç›‘æ§é…ç½®ã€‘=================
class FileMonitorConfig:
    ENABLE = True
    SOURCE_DIR = r"V:\Sony-2"
    TRANSCRIPT_DIR = r"V:\Sony-2\transcripts"
    PROCESSED_DIR = r"V:\Sony-2\processed"
    MONITOR_INTERVAL = 3  # ç§’
    SUPPORTED_EXTENSIONS = ('.m4a', '.acc', '.aac', '.mp3', '.wav', '.ogg', '.flac')

# =================ã€è¾…åŠ©å‡½æ•°ã€‘=================
def clean_sensevoice_tags(text):
    """æ¸…ç†SenseVoiceæ ‡ç­¾"""
    if not text:
        return ""
    cleaned = re.sub(r'<\|.*?\|>', '', text)
    return cleaned.strip()

def format_time(ms):
    """æ ¼å¼åŒ–æ—¶é—´ï¼ˆæ¯«ç§’è½¬hh:mm:ssï¼‰"""
    seconds = ms / 1000
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h):02}:{int(m):02}:{int(s):02}"

def convert_audio_to_wav(audio_path, wav_path):
    """å°†éŸ³é¢‘è½¬æ¢ä¸ºWAVæ ¼å¼"""
    FFMPEG_PATH = "ffmpeg"
    command = [
        FFMPEG_PATH, '-y', '-i', audio_path, '-vn', '-map', '0:a',
        '-ar', '16000', '-ac', '1', '-c:a', 'pcm_s16le', wav_path
    ]
    try:
        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
        return True
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr.decode('utf-8', 'ignore').strip() if e.stderr else "Unknown error"
        if "moov atom not found" in error_msg:
            logger.error(f"  [Convert Error] æ–‡ä»¶å·²æŸåæˆ–æœªå®Œæˆå½•åˆ¶ (moov atom not found)")
        elif "Decoding requested, but no decoder found" in error_msg:
            logger.error(f"  [Convert Error] æ–‡ä»¶ä¸åŒ…å«æœ‰æ•ˆçš„éŸ³é¢‘æµ")
        else:
            logger.error(f"  [Convert Error] ffmpegè½¬æ¢å¤±è´¥: ... {error_msg[-500:]}")
        return False
    except Exception as e:
        logger.error(f"  [Convert Error] {e}")
        return False

def save_transcript_txt(full_text, segments, txt_path):
    """ä¿å­˜è½¬å½•ç»“æœä¸ºTXTæ–‡ä»¶"""
    try:
        content_lines = []
        emo_map = {
            "happy": "ğŸ˜Šå¼€å¿ƒ", "sad": "ğŸ˜”æ‚²ä¼¤", "angry": "ğŸ˜¡ç”Ÿæ°”",
            "laughter": "ğŸ¤£å¤§ç¬‘", "fearful": "ğŸ˜¨å®³æ€•", "surprised": "ğŸ˜²æƒŠè®¶",
            "neutral": ""
        }
        content_lines.append(f"=== å…¨æ–‡æ‘˜è¦ ===\n{full_text}\n")
        content_lines.append("=== å¯¹è¯è®°å½• (æŒ‰è¯´è¯äºº) ===")
        for seg in segments:
            start_str = format_time(seg.get('start', 0))
            spk_label = str(seg.get('spk', 'Unknown'))
            emotion_key = seg.get('emotion', 'neutral')
            emo_str = emo_map.get(emotion_key, "")
            if emo_str:
                emo_str = f" {emo_str}"
            text = clean_sensevoice_tags(seg.get('text', '').strip())
            if not text:
                continue
            line = f"[{start_str}] [{spk_label}]{emo_str}: {text}"
            content_lines.append(line)
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("\n\n".join(content_lines))
        return True
    except Exception as e:
        logger.error(f"  [Save TXT Error] {e}")
        return False

# =================ã€å†…éƒ¨è½¬å½•å‡½æ•°ã€‘=================
def transcribe_internal(wav_path):
    """
    å†…éƒ¨è½¬å½•å‡½æ•° - ç›´æ¥è°ƒç”¨asr_serverçš„è½¬å½•é€»è¾‘
    
    Args:
        wav_path: WAVéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: è½¬å½•ç»“æœ {"full_text": str, "segments": list, "meta": dict}
        None: è½¬å½•å¤±è´¥
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°è½¬å½•ä»»åŠ¡: {os.path.basename(wav_path)}")
        
        # è°ƒç”¨asr_serveræ¨¡å—çš„è½¬å½•endpointçš„æ ¸å¿ƒé€»è¾‘
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä»asr_server.pyçš„transcribe_audioå‡½æ•°ä¸­æå–æ ¸å¿ƒé€»è¾‘
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥å¯¼å…¥å¿…è¦çš„å˜é‡å¹¶å¤åˆ¶éƒ¨åˆ†é€»è¾‘
        
        from asr_server import asr_pipeline, identify_speaker_fusion, gpu_lock, EMOTION_TAGS, INVALID_TAGS
        
        if not os.path.exists(wav_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {wav_path}")
            return None
        
        # é¢„å¤„ç†éŸ³é¢‘ - ç¡®ä¿åœ¨tempç›®å½•åˆ›å»º
        # ä»wav_pathæå–æ–‡ä»¶åï¼Œåœ¨tempç›®å½•åˆ›å»ºprocessedæ–‡ä»¶
        temp_dir = os.path.dirname(wav_path)  # wav_pathå·²ç»åœ¨tempç›®å½•ä¸­
        wav_basename = os.path.basename(wav_path)
        processed_path = os.path.join(temp_dir, wav_basename.replace("_TEMP.wav", "_TEMP.processed.wav"))
        
        if not preprocess_audio(wav_path, processed_path):
            logger.error("éŸ³é¢‘é¢„å¤„ç†å¤±è´¥")
            return None
        
        # è·å–éŸ³é¢‘æ—¶é•¿
        import torchaudio
        waveform, sr = torchaudio.load(processed_path)
        audio_duration = waveform.shape[1] / sr
        
        logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 1. ASRè¯†åˆ«] å¼€å§‹...")
        start_time = time.time()
        
        with gpu_lock:
            res = asr_pipeline.generate(
                input=processed_path,
                batch_size_s=300,
                hotword='é­”éƒ½'
            )
        
        process_time = time.time() - start_time
        
        segments = []
        full_text = ""
        
        if res and isinstance(res, list) and len(res) > 0:
            item = res[0]
            full_text = item.get("text", "")
            
            raw_segments = item.get("sentence_info", [])
            logger.info(f"  [ç”Ÿå‘½å‘¨æœŸ: 2. VAD & ASR] å®Œæˆ, VADæ£€å‡º {len(raw_segments)} ä¸ªåˆ†æ®µã€‚")
            
            if not raw_segments and full_text:
                raw_segments = [{"text": full_text, "start": 0, "end": int(audio_duration * 1000)}]
            
            processed_segments = []
            
            if raw_segments:
                logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 3. é€æ®µå£°çº¹è¯†åˆ«] å¼€å§‹...")
                for i, seg in enumerate(raw_segments):
                    seg_text = seg.get("text", "").strip()
                    start = seg.get("start", 0)
                    end = seg.get("end", 0)
                    
                    # æ¸…ç†emotion/eventæ ‡ç­¾
                    emotion = "neutral"
                    emotion_source = "funasr"  # é»˜è®¤æ¥æº
                    original_emotion_tag = None
                    
                    for tag, emo in EMOTION_TAGS.items():
                        if tag in seg_text:
                            emotion = emo
                            emotion_source = "funasr"
                            original_emotion_tag = tag
                            seg_text = seg_text.replace(tag, "")
                    
                    for tag in INVALID_TAGS:
                        seg_text = seg_text.replace(tag, "")
                    
                    clean_text = seg_text.strip()
                    
                    if not clean_text:
                        # è·³è¿‡ç©ºæ–‡æœ¬ï¼ˆé™é»˜ï¼‰
                        logger.debug(f"  #{i+1} è·³è¿‡: æ–‡æœ¬ä¸ºç©º")
                        continue
                    
                    duration_ms = end - start
                    if duration_ms < ASRConfig.MIN_SPEAKER_DURATION_MS:
                        # è·³è¿‡æ—¶é•¿ä¸è¶³ï¼ˆé™é»˜ï¼‰
                        logger.debug(f"  #{i+1} è·³è¿‡: æ—¶é•¿{duration_ms}ms < {ASRConfig.MIN_SPEAKER_DURATION_MS}ms")
                        continue
                    
                    # å£°çº¹è¯†åˆ«
                    identity = None
                    confidence = 0.0
                    recognition_details = {}
                    whisper_text = None
                    sensevoice_text = None
                    
                    segment_path = processed_path + f".seg_{i}.wav"
                    try:
                        from asr_server import extract_segment, transcribe_with_whisper, transcribe_with_sensevoice
                        if extract_segment(processed_path, start, end, segment_path):
                            result = identify_speaker_fusion(segment_path)
                            if result:
                                identity, confidence, recognition_details = result
                            
                            # æ€§èƒ½ä¼˜åŒ–: åªæœ‰è¯†åˆ«å‡ºçš„è¯´è¯äººæ‰è¿›è¡ŒWhisperå’ŒSenseVoiceå¤„ç†
                            if identity is not None:
                                # Whisperå¯¹æ¯”è¯†åˆ«
                                whisper_text = transcribe_with_whisper(segment_path)
                                
                                # SenseVoiceè¯†åˆ«å’Œæƒ…æ„Ÿæ£€æµ‹
                                sensevoice_result = transcribe_with_sensevoice(segment_path)
                                if sensevoice_result:
                                    sensevoice_text, sensevoice_emotion = sensevoice_result
                                    # ä½¿ç”¨SenseVoiceçš„æƒ…æ„Ÿç»“æœ(å¦‚æœæ£€æµ‹åˆ°)
                                    if sensevoice_emotion is not None:
                                        emotion = sensevoice_emotion
                                        emotion_source = "sensevoice"
                                        original_emotion_tag = f"<|{sensevoice_emotion}|>"
                                
                                # è¯†åˆ«æˆåŠŸï¼ˆé™é»˜ï¼‰
                                logger.debug(f"  #{i+1} è¯†åˆ«: {identity} ({confidence:.3f})")
                            else:
                                # æœªè¯†åˆ«ï¼ˆé™é»˜ï¼‰
                                logger.debug(f"  #{i+1} æœªè¯†åˆ«")
                                
                    except Exception as e:
                        logger.warning(f"      [3.{i+1}] å£°çº¹è¯†åˆ«å‡ºé”™: {e}")
                    finally:
                        if os.path.exists(segment_path):
                            try:
                                os.remove(segment_path)
                            except:
                                pass
                    
                    # æ£€æµ‹æ˜¯å¦ä¸ºå™ªéŸ³(é‡å¤å­—ç¬¦è¿‡å¤šæˆ–å¡«å……è¯)
                    def is_noise(text):
                        if not text:
                            return True
                        # æ£€æµ‹å•å­—ç¬¦é‡å¤ç‡
                        from collections import Counter
                        char_counts = Counter(text)
                        most_common_char, most_common_count = char_counts.most_common(1)[0]
                        repeat_ratio = most_common_count / len(text)
                        # å¦‚æœæŸä¸ªå­—ç¬¦å æ¯”è¶…è¿‡40%,è®¤ä¸ºæ˜¯å™ªéŸ³
                        if repeat_ratio > 0.4:
                            return True
                        
                        # æ£€æµ‹å¡«å……è¯(å—¯ã€å•Šã€å‘ƒç­‰)
                        filler_words = ['å—¯', 'å•Š', 'å‘ƒ', 'é¢', 'å“¦', 'å””']
                        # ç§»é™¤æ ‡ç‚¹åæ£€æŸ¥
                        text_no_punct = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼Ÿ,.!?]', '', text)
                        if not text_no_punct:
                            return True
                        # è®¡ç®—å¡«å……è¯å æ¯”
                        filler_count = sum(text_no_punct.count(w) for w in filler_words)
                        filler_ratio = filler_count / len(text_no_punct)
                        # å¦‚æœå¡«å……è¯å æ¯”è¶…è¿‡60%,è®¤ä¸ºæ˜¯å™ªéŸ³
                        return filler_ratio > 0.6
                    
                    # è®¡ç®—æ–‡æœ¬è´¨é‡æŒ‡æ ‡
                    def calculate_text_quality(text):
                        """è®¡ç®—æ–‡æœ¬è´¨é‡è¯„ä¼°æŒ‡æ ‡"""
                        from collections import Counter
                        if not text:
                            return {
                                "is_noise": True,
                                "noise_score": 1.0,
                                "repeat_ratio": 0.0,
                                "filler_ratio": 0.0
                            }
                        
                        # è®¡ç®—é‡å¤å­—ç¬¦å æ¯”
                        char_counts = Counter(text)
                        most_common_char, most_common_count = char_counts.most_common(1)[0]
                        repeat_ratio = most_common_count / len(text)
                        
                        # è®¡ç®—å¡«å……è¯å æ¯”
                        filler_words = ['å—¯', 'å•Š', 'å‘ƒ', 'é¢', 'å“¦', 'å””']
                        text_no_punct = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼Ÿ,.!?]', '', text)
                        filler_count = sum(text_no_punct.count(w) for w in filler_words) if text_no_punct else 0
                        filler_ratio = filler_count / len(text_no_punct) if text_no_punct else 0
                        
                        # ç»¼åˆå™ªéŸ³è¯„åˆ† (0-1, è¶Šé«˜è¶Šå¯èƒ½æ˜¯å™ªéŸ³)
                        noise_score = (repeat_ratio * 0.6 + filler_ratio * 0.4)
                        is_noise_flag = repeat_ratio > 0.4 or filler_ratio > 0.6
                        
                        return {
                            "is_noise": is_noise_flag,
                            "noise_score": round(noise_score, 3),
                            "repeat_ratio": round(repeat_ratio, 3),
                            "filler_ratio": round(filler_ratio, 3)
                        }
                    
                    # è¿‡æ»¤å™ªéŸ³
                    if is_noise(clean_text):
                        # è·³è¿‡å™ªéŸ³ï¼ˆé™é»˜ï¼‰
                        logger.debug(f"  #{i+1} è·³è¿‡: å™ªéŸ³")
                        continue
                    
                    # åªä¿ç•™å·²æ³¨å†Œè¯´è¯äºº,ä¸¢å¼ƒUnknown
                    if ASRConfig.ONLY_REGISTERED_SPEAKERS and identity is None:
                        # è·³è¿‡æœªè¯†åˆ«ï¼ˆé™é»˜ï¼‰
                        logger.debug(f"  #{i+1} è·³è¿‡: æœªè¯†åˆ«è¯´è¯äºº")
                        continue
                    
                    # è®¡ç®—è¯­é€ŸæŒ‡æ ‡
                    duration_seconds = duration_ms / 1000.0
                    word_count = len(clean_text)  # ä¸­æ–‡æŒ‰å­—ç¬¦æ•°è®¡ç®—
                    speech_rate = word_count / duration_seconds if duration_seconds > 0 else 0
                    
                    # è®¡ç®—æ–‡æœ¬è´¨é‡
                    text_quality = calculate_text_quality(clean_text)
                    
                    processed_segments.append({
                        # === åŸæœ‰å­—æ®µï¼ˆä¿æŒä¸å˜ï¼‰===
                        "text": clean_text,
                        "start": start,
                        "end": end,
                        "spk": identity or "Unknown",
                        "emotion": emotion,
                        "whisper_text": whisper_text,
                        "sensevoice_text": sensevoice_text,
                        "confidence": float(f"{confidence:.3f}"),
                        "recognition_details": recognition_details,
                        
                        # === æ–°å¢å­—æ®µï¼šè¯­é€ŸæŒ‡æ ‡ ===
                        "speech_metrics": {
                            "duration_seconds": round(duration_seconds, 2),
                            "word_count": word_count,
                            "speech_rate": round(speech_rate, 2)  # å­—/ç§’
                        },
                        
                        # === æ–°å¢å­—æ®µï¼šæ–‡æœ¬è´¨é‡è¯„ä¼° ===
                        "text_quality": text_quality,
                        
                        # === æ–°å¢å­—æ®µï¼šæƒ…æ„Ÿè¯¦ç»†ä¿¡æ¯ ===
                        "emotion_info": {
                            "emotion": emotion,
                            "source": emotion_source,  # "funasr" æˆ– "sensevoice"
                            "original_tag": original_emotion_tag,  # åŸå§‹æƒ…æ„Ÿæ ‡ç­¾ï¼Œå¦‚ "<|happy|>"
                            "detected_by_sensevoice": emotion_source == "sensevoice"
                        }
                    })
                
                logger.info("  [ç”Ÿå‘½å‘¨æœŸ: 3. é€æ®µå£°çº¹è¯†åˆ«] å®Œæˆã€‚")
            
            segments = processed_segments
            full_text = "".join([s["text"] for s in segments])
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(processed_path):
            processed_basename = os.path.basename(processed_path)
            try:
                os.remove(processed_path)
                logger.debug(f"  [Cleanup] å·²åˆ é™¤å¤„ç†åæ–‡ä»¶: {processed_basename}")
            except Exception as e:
                logger.warning(f"  [Cleanup] åˆ é™¤å¤„ç†åæ–‡ä»¶å¤±è´¥: {processed_basename}, é”™è¯¯: {e}")
        
        rtf = process_time / audio_duration if audio_duration > 0 else 0
        
        result = {
            "full_text": full_text,
            "segments": segments,
            "meta": {
                "audio_duration": audio_duration,
                "process_time": process_time,
                "rtf": rtf,
                "rtf_description": "Real-Time Factor(å®æ—¶å› å­)ï¼Œå¤„ç†æ—¶é—´/éŸ³é¢‘æ—¶é•¿ï¼ŒRTF < 1è¡¨ç¤ºå¯å®æ—¶å¤„ç†ï¼Œå€¼è¶Šä½æ€§èƒ½è¶Šå¥½"
            }                        }
        
        logger.info(f"âœ… è½¬å½•å®Œæˆ: {len(segments)} ä¸ªåˆ†æ®µ, RTF={rtf:.3f}")
        return result
        
    except Exception as e:
        logger.error(f"è½¬å½•å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return None

# =================ã€æ–‡ä»¶ç›‘æ§å¾ªç¯ã€‘=================
def process_one_file(filename):
    """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
    source_path = os.path.join(FileMonitorConfig.SOURCE_DIR, filename)
    
    logger.info(f"\n>>> å¤„ç†: {filename}")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼ˆä½¿ç”¨asr-serverçš„tempç›®å½•ï¼‰
    temp_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "temp")
    os.makedirs(temp_dir, exist_ok=True)
    
    # è½¬æ¢ä¸ºWAVï¼ˆä½¿ç”¨tempç›®å½•ï¼‰
    base_filename = os.path.basename(filename)
    wav_path = os.path.join(temp_dir, base_filename + "_TEMP.wav")
    if not convert_audio_to_wav(source_path, wav_path):
        logger.error("  éŸ³é¢‘è½¬æ¢å¤±è´¥ï¼Œè·³è¿‡")
        return False
    
    try:
        # å†…éƒ¨è½¬å½•ï¼ˆç›´æ¥å‡½æ•°è°ƒç”¨ï¼‰
        result = transcribe_internal(wav_path)
        
        if not result:
            logger.error("  è½¬å½•å¤±è´¥")
            return False
        
        full_text = result.get("full_text", "")
        segments = result.get("segments", [])
        
        if not segments:
            logger.warning(f"  æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³åˆ†æ®µ")
            return False
        
        logger.info(f"  è½¬å½•æˆåŠŸ: {len(segments)} ä¸ªåˆ†æ®µ")
        
        # è§£æå½•éŸ³æ—¶é—´
        from db_manager import parse_recording_time
        recording_time = parse_recording_time(filename)
        
        # æå–éŸ³é¢‘ç‰‡æ®µå¹¶æ·»åŠ segment_audio_path
        base_name = os.path.splitext(filename)[0]
        segments_dir = os.path.join(FileMonitorConfig.SOURCE_DIR, "audio_segments", base_name)
        os.makedirs(segments_dir, exist_ok=True)
        logger.info(f"  [Audio Segments] åˆ›å»ºç‰‡æ®µç›®å½•: {segments_dir}")
        
        # å¯¼å…¥éŸ³é¢‘æå–å‡½æ•°
        from asr_server import extract_segment
        
        updated_segments = []
        for i, seg in enumerate(segments):
            # æå–éŸ³é¢‘ç‰‡æ®µ
            seg_filename = f"seg_{i}.wav"
            seg_path = os.path.join(segments_dir, seg_filename)
            seg_audio_path = f"/audio_segments/{base_name}/{seg_filename}"
            
            start_ms = seg.get("start", 0)
            end_ms = seg.get("end", 0)
            
            # å°è¯•æå–éŸ³é¢‘ç‰‡æ®µ
            if extract_segment(wav_path, start_ms, end_ms, seg_path):
                logger.info(f"  [Audio Segments] æå–ç‰‡æ®µ {i}: {start_ms}ms - {end_ms}ms â†’ {seg_path}")
            else:
                logger.warning(f"  [Audio Segments] ç‰‡æ®µ {i} æå–å¤±è´¥")
                seg_audio_path = None
            
            # ä¿ç•™æ‰€æœ‰åŸå§‹å­—æ®µå¹¶æ·»åŠ segment_audio_path
            original_path = seg.get("segment_audio_path")
            segment_data = seg.copy()
            segment_data["segment_audio_path"] = seg_audio_path
            
            # æ—¥å¿—è¿½è¸ªè·¯å¾„å˜åŒ–
            if original_path:
                logger.info(f"  [Path Override] ç‰‡æ®µ {i}: '{original_path}' â†’ '{seg_audio_path}'")
            else:
                logger.info(f"  [Path Set] ç‰‡æ®µ {i}: '{seg_audio_path}'")
            
            updated_segments.append(segment_data)
        
        segments = updated_segments
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        try:
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥æ± æ˜¯å¦åˆå§‹åŒ–
            from db_manager import connection_pool
            if not connection_pool:
                logger.error(f"  æ•°æ®åº“è¿æ¥æ± æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
            else:
                success = save_to_db(filename, full_text, segments, recording_time)
                if success:
                    logger.info(f"  æ•°æ®åº“ä¿å­˜æˆåŠŸ (recording_time: {recording_time})")
                else:
                    logger.error(f"  æ•°æ®åº“ä¿å­˜å¤±è´¥: save_to_dbè¿”å›False")
        except Exception as e:
            logger.error(f"  æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
        
        # ä¿å­˜TXTæ–‡ä»¶
        txt_filename = os.path.splitext(filename)[0] + ".txt"
        txt_path = os.path.join(FileMonitorConfig.TRANSCRIPT_DIR, txt_filename)
        os.makedirs(FileMonitorConfig.TRANSCRIPT_DIR, exist_ok=True)
        
        if save_transcript_txt(full_text, segments, txt_path):
            logger.info(f"  TXTå·²ä¿å­˜")
        else:
            logger.warning(f"  TXTä¿å­˜å¤±è´¥")
        
        # ç§»åŠ¨åˆ°processedç›®å½•
        os.makedirs(FileMonitorConfig.PROCESSED_DIR, exist_ok=True)
        dest_path = os.path.join(FileMonitorConfig.PROCESSED_DIR, filename)
        shutil.move(source_path, dest_path)
        logger.info(f"  å·²ç§»åŠ¨åˆ°: processed/{filename}")
        
        return True
        
    finally:
        # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼ˆä»tempç›®å½•ï¼‰
        if os.path.exists(wav_path):
            try:
                os.remove(wav_path)
                logger.info(f"  [Cleanup] å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {os.path.basename(wav_path)}")
            except Exception as e:
                logger.warning(f"  [Cleanup] åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ¸…ç†.processed.wavæ–‡ä»¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„è·¯å¾„ï¼‰
        temp_dir = os.path.dirname(wav_path)
        wav_basename = os.path.basename(wav_path)
        processed_path = os.path.join(temp_dir, wav_basename.replace("_TEMP.wav", "_TEMP.processed.wav"))
        
        if os.path.exists(processed_path):
            try:
                os.remove(processed_path)
                logger.info(f"  [Cleanup] å·²åˆ é™¤å¤„ç†åæ–‡ä»¶: {os.path.basename(processed_path)}")
            except Exception as e:
                logger.warning(f"  [Cleanup] åˆ é™¤å¤„ç†åæ–‡ä»¶å¤±è´¥: {e}")


def file_monitor_loop():
    """æ–‡ä»¶ç›‘æ§ä¸»å¾ªç¯"""
    logger.info(f"\nğŸ“ æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨")
    logger.info(f"   ç›‘æ§ç›®å½•: {FileMonitorConfig.SOURCE_DIR}")
    logger.info(f"   æ‰«æé—´éš”: {FileMonitorConfig.MONITOR_INTERVAL}ç§’\n")
    
    while True:
        try:
            if not os.path.exists(FileMonitorConfig.SOURCE_DIR):
                logger.warning(f"æºç›®å½•ä¸å­˜åœ¨: {FileMonitorConfig.SOURCE_DIR}")
                time.sleep(FileMonitorConfig.MONITOR_INTERVAL)
                continue
            
            files = [
                f for f in os.listdir(FileMonitorConfig.SOURCE_DIR)
                if f.lower().endswith(FileMonitorConfig.SUPPORTED_EXTENSIONS)
            ]
            
            if files:
                logger.info(f"å‘ç° {len(files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
                for filename in files:
                    process_one_file(filename)
            
            time.sleep(FileMonitorConfig.MONITOR_INTERVAL)
            
        except KeyboardInterrupt:
            logger.info("æ–‡ä»¶ç›‘æ§åœæ­¢")
            break
        except Exception as e:
            logger.error(f"æ–‡ä»¶ç›‘æ§å‡ºé”™: {e}")
            time.sleep(10)

# =================ã€ä¸»å‡½æ•°ã€‘=================
if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ ç»Ÿä¸€ASRæœåŠ¡å¯åŠ¨ä¸­...")
    print("="*60 + "\n")
    
    # 1. åˆå§‹åŒ–æ•°æ®åº“
    if FileMonitorConfig.ENABLE:
        logger.info("åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        if not init_pool():
            logger.error("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥")
            sys.exit(1)
        init_db()
    
    # 2. åŠ è½½AIæ¨¡å‹
    asr_server.load_models()
    
    # 3. å¯åŠ¨æ–‡ä»¶ç›‘æ§çº¿ç¨‹
    if FileMonitorConfig.ENABLE:
        monitor_thread = threading.Thread(target=file_monitor_loop, daemon=True)
        monitor_thread.start()
        logger.info("âœ… æ–‡ä»¶ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨\n")
    
    # 4. å¯åŠ¨FlaskæœåŠ¡
    try:
        logger.info(f"ğŸŒ å¯åŠ¨HTTPæœåŠ¡: http://{ASRConfig.HOST}:{ASRConfig.PORT}\n")
        asr_server.app.run(
            host=ASRConfig.HOST,
            port=ASRConfig.PORT,
            debug=False,
            threaded=True
        )
    except KeyboardInterrupt:
        logger.info("\næ­£åœ¨å…³é—­æœåŠ¡...")
    finally:
        if FileMonitorConfig.ENABLE:
            close_pool()
        logger.info("æœåŠ¡å·²åœæ­¢")
