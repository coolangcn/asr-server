#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ASRæœåŠ¡ - æ•´åˆæ–‡ä»¶ç›‘æ§å’Œè½¬å½•åŠŸèƒ½
"""

import os
import sys
import time
import threading
import subprocess
import shutil
import re
import json
import traceback
import requests  # ç”¨äºè°ƒç”¨ /transcribe API
from datetime import datetime

# å¯¼å…¥ASRæœåŠ¡æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
import asr_server
from asr_server import Config as ASRConfig, logger
from db_manager import init_pool, init_db, save_to_db, close_pool, update_topics

# =================ã€æ–‡ä»¶ç›‘æ§é…ç½®ã€‘=================
class FileMonitorConfig:
    ENABLE = True
    SOURCE_DIR = r"V:\Sony-2"
    TRANSCRIPT_DIR = r"V:\Sony-2\transcripts"
    PROCESSED_DIR = r"V:\Sony-2\processed"
    MONITOR_INTERVAL = 3  # ç§’
    SUPPORTED_EXTENSIONS = ('.m4a', '.acc', '.aac', '.mp3', '.wav', '.ogg', '.flac')

# =================ã€LLM é…ç½®ã€‘=================
class LLMConfig:
    # Gemini API é…ç½®
    GEMINI_API_KEY = "cncncncn"
    USE_GEMINI_LLM = True
    GEMINI_API_BASE_URL = "https://gl.moco.fun/proxy/gemini"
    GEMINI_MODEL_NAME = "gemini-2.5-flash"
    
    # æ‰¹é‡å¤„ç†é…ç½®
    LLM_BATCH_MODE = True
    LLM_BATCH_SIZE = 20       # æ”’å¤Ÿ20æ¡å¯¹è¯åæ‰¹é‡å¤„ç†ï¼ˆèŠ‚çœ95%æˆæœ¬ï¼‰
    LLM_BATCH_TIMEOUT = 600   # 10åˆ†é’Ÿè¶…æ—¶
    
    # è¿‡æ»¤æ¡ä»¶
    LLM_MIN_TEXT_LENGTH = 50
    LLM_MIN_SEGMENTS = 3

# =================ã€LLM æ‰¹é‡å¤„ç†ã€‘=================
# æ‰¹é‡å¤„ç†é˜Ÿåˆ—ï¼ˆæ”’å¤Ÿä¸€å®šæ•°é‡å†è°ƒç”¨ LLMï¼‰
llm_batch_queue = []  # [(filename, full_text, segments), ...]
llm_batch_lock = threading.Lock()
llm_batch_timer = None

def process_llm_batch():
    """æ‰¹é‡å¤„ç† LLM é˜Ÿåˆ—"""
    global llm_batch_timer
    
    with llm_batch_lock:
        if not llm_batch_queue:
            llm_batch_timer = None
            return
        batch = llm_batch_queue[:]
        llm_batch_queue.clear()
        llm_batch_timer = None
    
    logger.info(f"[LLM æ‰¹é‡] å¼€å§‹å¤„ç† {len(batch)} æ¡å¯¹è¯")
    for filename, full_text, segments in batch:
        try:
            topics = extract_conversation_topics(full_text, segments)
            if topics:
                logger.info(f"[LLM æ‰¹é‡] {filename} ä¸»é¢˜: {topics.get('topics', [])}")
                # æ›´æ–°æ•°æ®åº“
                update_topics(filename, topics)
        except Exception as e:
            logger.error(f"[LLM æ‰¹é‡] {filename} å¤±è´¥: {e}")
    logger.info(f"[LLM æ‰¹é‡] å®Œæˆ")

def add_to_llm_queue(filename, full_text, segments):
    """æ·»åŠ åˆ° LLM æ‰¹é‡é˜Ÿåˆ—"""
    global llm_batch_timer
    
    if not LLMConfig.LLM_BATCH_MODE:
        return
    
    with llm_batch_lock:
        llm_batch_queue.append((filename, full_text, segments))
        size = len(llm_batch_queue)
        logger.info(f"[LLM é˜Ÿåˆ—] +1, å½“å‰: {size}/{LLMConfig.LLM_BATCH_SIZE}")
        
        if size >= LLMConfig.LLM_BATCH_SIZE:
            if llm_batch_timer:
                llm_batch_timer.cancel()
            threading.Thread(target=process_llm_batch, daemon=True).start()
        elif not llm_batch_timer:
            llm_batch_timer = threading.Timer(LLMConfig.LLM_BATCH_TIMEOUT, process_llm_batch)
            llm_batch_timer.daemon = True
            llm_batch_timer.start()

def call_gemini_api(prompt, max_tokens=500):
    """è°ƒç”¨ Gemini API"""
    if not LLMConfig.USE_GEMINI_LLM or not LLMConfig.GEMINI_API_KEY:
        return None
    
    try:
        import requests
        api_url = f"{LLMConfig.GEMINI_API_BASE_URL}/v1beta/models/{LLMConfig.GEMINI_MODEL_NAME}:generateContent"
        
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": LLMConfig.GEMINI_API_KEY
        }
        
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"maxOutputTokens": max_tokens, "temperature": 0.7}
        }
        
        response = requests.post(api_url, json=payload, headers=headers, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
            return text.strip()
        else:
            logger.error(f"Gemini API å¤±è´¥: {response.status_code}")
            return None
    except Exception as e:
        logger.error(f"Gemini API å¼‚å¸¸: {e}")
        return None

def extract_conversation_topics(full_text, segments):
    """ä½¿ç”¨ Gemini æå–å¯¹è¯ä¸»é¢˜"""
    if not LLMConfig.USE_GEMINI_LLM or len(full_text) < 10:
        return None
    
    try:
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

å¯¹è¯å†…å®¹ï¼š
{full_text[:500]}

è¯·ä»¥JSONæ ¼å¼è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š
{{
  "topics": ["ä¸»é¢˜1", "ä¸»é¢˜2"],
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
  "sentiment": "positive/neutral/negative",
  "summary": "ä¸€å¥è¯æ€»ç»“å¯¹è¯å†…å®¹"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        response = call_gemini_api(prompt, max_tokens=300)
        
        if response:
            import json
            json_text = response
            if '```json' in response:
                json_text = response.split('```json')[1].split('```')[0]
            elif '```' in response:
                json_text = response.split('```')[1].split('```')[0]
            
            result = json.loads(json_text.strip())
            return result
        
        return None
    except Exception as e:
        logger.warning(f"ä¸»é¢˜æå–å¤±è´¥: {e}")
        return None

def generate_conversation_summary(segments, audio_duration):
    """ç”Ÿæˆå¯¹è¯æ™ºèƒ½æ‘˜è¦"""
    if not segments:
        return None
    
    from collections import Counter
    
    # ç»Ÿè®¡è¯´è¯äºº
    speaker_stats = {}
    for seg in segments:
        speaker = seg.get('spk', 'Unknown')
        if speaker not in speaker_stats:
            speaker_stats[speaker] = {'count': 0, 'total_duration': 0, 'word_count': 0}
        speaker_stats[speaker]['count'] += 1
        speaker_stats[speaker]['total_duration'] += (seg.get('end', 0) - seg.get('start', 0)) / 1000.0
        speaker_stats[speaker]['word_count'] += len(seg.get('text', ''))
    
    # æå–é«˜é¢‘è¯
    stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'æœ‰', 'ä¸ª', 'å°±', 'ä¸', 'å’Œ', 'ä¸'}
    all_text = ''.join([seg.get('text', '') for seg in segments])
    words = [all_text[i:i+2] for i in range(len(all_text)-1)]
    word_freq = Counter([w for w in words if w not in stop_words and len(w) == 2])
    top_keywords = [word for word, count in word_freq.most_common(5)]
    
    # æƒ…æ„Ÿç»Ÿè®¡
    emotion_stats = Counter([seg.get('emotion') for seg in segments if seg.get('emotion')])
    
    return {
        'total_segments': len(segments),
        'total_duration': round(audio_duration, 2),
        'speaker_count': len(speaker_stats),
        'speakers': speaker_stats,
        'keywords': top_keywords,
        'emotions': dict(emotion_stats),
        'avg_segment_duration': round(audio_duration / len(segments), 2) if segments else 0
    }

# =================ã€è¾…åŠ©å‡½æ•°ã€‘=================
def clean_sensevoice_tags(text):
    """æ¸…ç†SenseVoiceæ ‡ç­¾"""
    if not text:
        return ""
    cleaned = re.sub(r'<\|.*?\|>', '', text)
    return cleaned.strip()

def format_time(ms):
    """æ ¼å¼åŒ–æ—¶é—´ï¼ˆæ¯«ç§’è½¬hh:mm:ssï¼‰"""
    seconds = ms / 1000
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h):02}:{int(m):02}:{int(s):02}"

def convert_audio_to_wav(audio_path, wav_path):
    """å°†éŸ³é¢‘è½¬æ¢ä¸ºWAVæ ¼å¼"""
    FFMPEG_PATH = "ffmpeg"
    command = [
        FFMPEG_PATH, '-y', '-i', audio_path, '-vn', '-map', '0:a',
        '-ar', '16000', '-ac', '1', '-c:a', 'pcm_s16le', wav_path
    ]
    try:
        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
        return True
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr.decode('utf-8', 'ignore').strip() if e.stderr else "Unknown error"
        if "moov atom not found" in error_msg:
            logger.error(f"  [Convert Error] æ–‡ä»¶å·²æŸåæˆ–æœªå®Œæˆå½•åˆ¶ (moov atom not found)")
        elif "Decoding requested, but no decoder found" in error_msg:
            logger.error(f"  [Convert Error] æ–‡ä»¶ä¸åŒ…å«æœ‰æ•ˆçš„éŸ³é¢‘æµ")
        else:
            logger.error(f"  [Convert Error] ffmpegè½¬æ¢å¤±è´¥: ... {error_msg[-500:]}")
        return False
    except Exception as e:
        logger.error(f"  [Convert Error] {e}")
        return False

def save_transcript_txt(full_text, segments, txt_path):
    """ä¿å­˜è½¬å½•ç»“æœä¸ºTXTæ–‡ä»¶"""
    try:
        content_lines = []
        emo_map = {
            "happy": "ğŸ˜Šå¼€å¿ƒ", "sad": "ğŸ˜”æ‚²ä¼¤", "angry": "ğŸ˜¡ç”Ÿæ°”",
            "laughter": "ğŸ¤£å¤§ç¬‘", "fearful": "ğŸ˜¨å®³æ€•", "surprised": "ğŸ˜²æƒŠè®¶",
            "neutral": ""
        }
        content_lines.append(f"=== å…¨æ–‡æ‘˜è¦ ===\n{full_text}\n")
        content_lines.append("=== å¯¹è¯è®°å½• (æŒ‰è¯´è¯äºº) ===")
        for seg in segments:
            start_str = format_time(seg.get('start', 0))
            spk_label = str(seg.get('spk', 'Unknown'))
            emotion_key = seg.get('emotion', 'neutral')
            emo_str = emo_map.get(emotion_key, "")
            if emo_str:
                emo_str = f" {emo_str}"
            text = clean_sensevoice_tags(seg.get('text', '').strip())
            if not text:
                continue
            line = f"[{start_str}] [{spk_label}]{emo_str}: {text}"
            content_lines.append(line)
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("\n\n".join(content_lines))
        return True
    except Exception as e:
        logger.error(f"  [Save TXT Error] {e}")
        return False

# =================ã€æ–‡ä»¶ç›‘æ§å¾ªç¯ã€‘=================
def process_one_file(filename):
    """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
    source_path = os.path.join(FileMonitorConfig.SOURCE_DIR, filename)
    
    logger.info(f"\n>>> å¤„ç†: {filename}")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼ˆä½¿ç”¨asr-serverçš„tempç›®å½•ï¼‰
    temp_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "temp")
    os.makedirs(temp_dir, exist_ok=True)
    
    # è½¬æ¢ä¸ºWAVï¼ˆä½¿ç”¨tempç›®å½•ï¼‰
    base_filename = os.path.basename(filename)
    wav_path = os.path.join(temp_dir, base_filename + "_TEMP.wav")
    if not convert_audio_to_wav(source_path, wav_path):
        logger.error("  éŸ³é¢‘è½¬æ¢å¤±è´¥ï¼Œè·³è¿‡")
        return False
    
    try:
        # è°ƒç”¨ /transcribe API
        api_url = f"http://localhost:{ASRConfig.PORT}/transcribe"
        
        with open(wav_path, 'rb') as f:
            files = {'audio_file': (os.path.basename(wav_path), f, 'audio/wav')}
            response = requests.post(api_url, files=files, timeout=300)
        
        if response.status_code != 200:
            logger.error(f"  APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            return False
        
        result = response.json()
        full_text = result.get("full_text", "")
        segments = result.get("segments", [])
        
        if not segments:
            logger.warning(f"  æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³åˆ†æ®µ")
            return False
        
        logger.info(f"  è½¬å½•æˆåŠŸ: {len(segments)} ä¸ªåˆ†æ®µ")
        
        # è§£æå½•éŸ³æ—¶é—´
        from db_manager import parse_recording_time
        recording_time = parse_recording_time(filename)
        
        # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
        audio_duration = sum((seg.get('end', 0) - seg.get('start', 0)) for seg in segments) / 1000.0
        summary = generate_conversation_summary(segments, audio_duration)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        try:
            from db_manager import connection_pool
            if not connection_pool:
                logger.error(f"  æ•°æ®åº“è¿æ¥æ± æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
            else:
                success = save_to_db(filename, full_text, segments, recording_time, summary)
                if success:
                    logger.info(f"  æ•°æ®åº“ä¿å­˜æˆåŠŸ (recording_time: {recording_time})")
                    if summary:
                        logger.info(f"  æ™ºèƒ½æ‘˜è¦: {summary['speaker_count']}ä½è¯´è¯äºº, {summary['total_segments']}ä¸ªåˆ†æ®µ, å…³é”®è¯: {summary['keywords']}")
                    
                    # æ·»åŠ åˆ° LLM æ‰¹é‡å¤„ç†é˜Ÿåˆ—ï¼ˆå¼‚æ­¥ï¼‰
                    if LLMConfig.USE_GEMINI_LLM:
                        has_identified_speakers = any(seg.get('spk') != 'Unknown' for seg in segments)
                        if (len(full_text) >= 50 and len(segments) >= 3 and has_identified_speakers):
                            add_to_llm_queue(filename, full_text, segments)
                else:
                    logger.error(f"  æ•°æ®åº“ä¿å­˜å¤±è´¥: save_to_dbè¿”å›False")
        except Exception as e:
            logger.error(f"  æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
        
        # ä¿å­˜TXTæ–‡ä»¶
        txt_filename = os.path.splitext(filename)[0] + ".txt"
        txt_path = os.path.join(FileMonitorConfig.TRANSCRIPT_DIR, txt_filename)
        os.makedirs(FileMonitorConfig.TRANSCRIPT_DIR, exist_ok=True)
        
        if save_transcript_txt(full_text, segments, txt_path):
            logger.info(f"  TXTå·²ä¿å­˜")
        else:
            logger.warning(f"  TXTä¿å­˜å¤±è´¥")
        
        # ç§»åŠ¨åˆ°processedç›®å½•
        os.makedirs(FileMonitorConfig.PROCESSED_DIR, exist_ok=True)
        dest_path = os.path.join(FileMonitorConfig.PROCESSED_DIR, filename)
        shutil.move(source_path, dest_path)
        logger.info(f"  å·²ç§»åŠ¨åˆ°: processed/{filename}")
        
        return True
        
    finally:
        # æ¸…ç†ä¸´æ—¶WAVæ–‡ä»¶
        if os.path.exists(wav_path):
            try:
                os.remove(wav_path)
                logger.info(f"  [Cleanup] å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {os.path.basename(wav_path)}")
            except Exception as e:
                logger.warning(f"  [Cleanup] åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")


def file_monitor_loop():
    """æ–‡ä»¶ç›‘æ§ä¸»å¾ªç¯"""
    logger.info(f"\nğŸ“ æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨")
    logger.info(f"   ç›‘æ§ç›®å½•: {FileMonitorConfig.SOURCE_DIR}")
    logger.info(f"   æ‰«æé—´éš”: {FileMonitorConfig.MONITOR_INTERVAL}ç§’\n")
    
    while True:
        try:
            if not os.path.exists(FileMonitorConfig.SOURCE_DIR):
                logger.warning(f"æºç›®å½•ä¸å­˜åœ¨: {FileMonitorConfig.SOURCE_DIR}")
                time.sleep(FileMonitorConfig.MONITOR_INTERVAL)
                continue
            
            files = [
                f for f in os.listdir(FileMonitorConfig.SOURCE_DIR)
                if f.lower().endswith(FileMonitorConfig.SUPPORTED_EXTENSIONS)
            ]
            
            if files:
                logger.info(f"å‘ç° {len(files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
                for filename in files:
                    process_one_file(filename)
            
            time.sleep(FileMonitorConfig.MONITOR_INTERVAL)
            
        except KeyboardInterrupt:
            logger.info("æ–‡ä»¶ç›‘æ§åœæ­¢")
            break
        except Exception as e:
            logger.error(f"æ–‡ä»¶ç›‘æ§å‡ºé”™: {e}")
            time.sleep(10)

# =================ã€ä¸»å‡½æ•°ã€‘=================
if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ ç»Ÿä¸€ASRæœåŠ¡å¯åŠ¨ä¸­...")
    print("="*60 + "\n")
    
    # 1. åˆå§‹åŒ–æ•°æ®åº“
    if FileMonitorConfig.ENABLE:
        logger.info("åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        if not init_pool():
            logger.error("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥")
            sys.exit(1)
        init_db()
    
    # 2. åŠ è½½AIæ¨¡å‹
    asr_server.load_models()
    
    # 3. å¯åŠ¨æ–‡ä»¶ç›‘æ§çº¿ç¨‹
    if FileMonitorConfig.ENABLE:
        monitor_thread = threading.Thread(target=file_monitor_loop, daemon=True)
        monitor_thread.start()
        logger.info("âœ… æ–‡ä»¶ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨\n")
    
    # 4. å¯åŠ¨FlaskæœåŠ¡
    try:
        logger.info(f"ğŸŒ å¯åŠ¨HTTPæœåŠ¡: http://{ASRConfig.HOST}:{ASRConfig.PORT}\n")
        asr_server.app.run(
            host=ASRConfig.HOST,
            port=ASRConfig.PORT,
            debug=False,
            threaded=True
        )
    except KeyboardInterrupt:
        logger.info("\næ­£åœ¨å…³é—­æœåŠ¡...")
    finally:
        if FileMonitorConfig.ENABLE:
            close_pool()
        logger.info("æœåŠ¡å·²åœæ­¢")
